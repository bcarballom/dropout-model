{"cells":[{"cell_type":"code","metadata":{"source_hash":"65cc8b16","execution_start":1752085889608,"execution_millis":12607,"execution_context_id":"7e54dcbd-e1df-4dd0-9b77-a9e7bed064b9","cell_id":"33532857466643f3ae52f6c7d49f02f6","deepnote_cell_type":"code"},"source":"# ================================================\n# MODELO PREDICTIVO DE DESERCIÓN UNIVERSITARIA\n# ================================================\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\nfrom sklearn.utils import resample\n\n# -------------------------------\n# 1. Cargar los datos\n# -------------------------------\ndf = pd.read_csv('data/variables.csv') \n\n# -------------------------------\n# 2. Filtrar cohortes útiles\n# -------------------------------\ndf = df[df['Año'] <= 2022]  # Solo usar cohortes con etiqueta conocida\n\n# -------------------------------\n# 3. Eliminar variables irrelevantes\n# -------------------------------\nexcluded_vars = [\n    'EMPLID', 'ETNIA', 'ADEUDO', 'PUNTAJE_ADM', 'CAMBIO_PROGRAMA', 'CARRERA',\n    'Año', 'SEMESTRE PLAN', 'ÚLTIMO PERIODO CURSADO', 'SEMESTRE ABANDONO', 'DESERCION'\n]\n\nX_full = df.drop(columns=excluded_vars, errors='ignore')\ny = df['DESERCION']\n\n# -------------------------------\n# 4. Eliminar variables no numéricas\n# -------------------------------\nnon_numeric_cols = X_full.select_dtypes(include=['object']).columns.tolist()\nX = X_full.drop(columns=non_numeric_cols, errors='ignore')\n\n# -------------------------------\n# 5. Dividir en entrenamiento y validación\n# -------------------------------\nX_train, X_val, y_train, y_val = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n\n# -------------------------------\n# 6. Balancear clases con submuestreo\n# -------------------------------\ntrain_data = X_train.copy()\ntrain_data['DESERCION'] = y_train\n\ndesertores = train_data[train_data['DESERCION'] == 1]\nno_desertores = train_data[train_data['DESERCION'] == 0]\n\nno_desertores_downsampled = resample(no_desertores,\n                                     replace=False,\n                                     n_samples=len(desertores),\n                                     random_state=42)\n\nbalanced_train_data = pd.concat([desertores, no_desertores_downsampled])\nX_train_bal = balanced_train_data.drop(columns='DESERCION')\ny_train_bal = balanced_train_data['DESERCION']\n\n# -------------------------------\n# 7. Escalamiento\n# -------------------------------\nscaler = StandardScaler()\nX_train_bal_scaled = scaler.fit_transform(X_train_bal)\nX_val_scaled = scaler.transform(X_val[X_train_bal.columns])  # Asegura mismas columnas\n\n# -------------------------------\n# 8. Definir modelos\n# -------------------------------\nmodels = {\n    \"Logistic Regression\": LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42),\n    \"Random Forest\": RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n}\n\n# -------------------------------\n# 9. Entrenar y evaluar modelos\n# -------------------------------\nfor name, model in models.items():\n    model.fit(X_train_bal_scaled, y_train_bal)\n    y_pred = model.predict(X_val_scaled)\n    y_proba = model.predict_proba(X_val_scaled)[:, 1]\n\n    print(f\"\\n===== {name} =====\")\n    print(\"ROC AUC:\", roc_auc_score(y_val, y_proba))\n    print(\"Confusion Matrix:\\n\", confusion_matrix(y_val, y_pred))\n    print(\"Classification Report:\\n\", classification_report(y_val, y_pred))\n","block_group":"33532857466643f3ae52f6c7d49f02f6","execution_count":1,"outputs":[{"name":"stdout","text":"\n===== Logistic Regression =====\nROC AUC: 0.690579843653216\nConfusion Matrix:\n [[3254 1632]\n [ 745 1219]]\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.81      0.67      0.73      4886\n           1       0.43      0.62      0.51      1964\n\n    accuracy                           0.65      6850\n   macro avg       0.62      0.64      0.62      6850\nweighted avg       0.70      0.65      0.67      6850\n\n\n===== Random Forest =====\nROC AUC: 0.7035457827468314\nConfusion Matrix:\n [[3175 1711]\n [ 686 1278]]\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.82      0.65      0.73      4886\n           1       0.43      0.65      0.52      1964\n\n    accuracy                           0.65      6850\n   macro avg       0.62      0.65      0.62      6850\nweighted avg       0.71      0.65      0.67      6850\n\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/e466e0b5-ef51-45cc-9021-86ce0f26a1d8","content_dependencies":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=24fa97d8-6b94-4f62-a2dc-97416a953ae1' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_notebook_id":"56db456adf224e63a322b6c71e88aef7"}}