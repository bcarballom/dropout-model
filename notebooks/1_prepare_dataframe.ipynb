{"cells":[{"cell_type":"code","metadata":{"source_hash":"afea176b","execution_start":1752122993392,"execution_millis":4155,"execution_context_id":"b4734e89-24fa-421e-a465-f05d90503f39","cell_id":"e565de86c0ac4889a75ca4f19b5b11f7","deepnote_cell_type":"code"},"source":"import pandas as pd\nimport os\n\n# 1) ACCEDER A LOS ARCHIVOS EN LA CARPETA ('DATA')\n\n# Definir la ruta donde se encuentran los archivos\nroot_path = os.getcwd()\nfolder_path = os.path.join(root_path,'data')   \n\n# Obtener una lista de todos los archivos en la carpeta\nfile_list = [file for file in os.listdir(folder_path) if file.endswith('.csv') and 'respuestas' in file]\n\n# Verificar que la lista de archivos no esté vacía\nif not file_list:\n    raise ValueError(\"No se encontraron archivos CSV que coincidan con el criterio especificado en la carpeta.\")\n\nprint(\"Archivos encontrados:\", file_list)","block_group":"e565de86c0ac4889a75ca4f19b5b11f7","execution_count":1,"outputs":[{"name":"stdout","text":"Archivos encontrados: ['2014-respuestas.csv', '2015-respuestas.csv', '2016-respuestas.csv', '2017-respuestas.csv', '2018-respuestas.csv', '2019-respuestas.csv', '2020-respuestas.csv', '2021-respuestas.csv', '2022-respuestas.csv', '2023-respuestas.csv', '2024-respuestas.csv']\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/805a229b-5e30-4e5a-a627-03075d6fa7f2","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"39dd2f4b","execution_start":1752122997607,"execution_millis":5474,"execution_context_id":"b4734e89-24fa-421e-a465-f05d90503f39","cell_id":"f97743a1685b4eaf9f7d59505945af29","deepnote_cell_type":"code"},"source":"# 2) CREAR EL DATAFRAME\n\nimport re\n\n# Obtener el dato del año del nombre del archivo con una función\ndef extract_year(file_name):\n    match = re.search(r'(\\d{4})', file_name)\n    return match.group(0) if match else None\n\n# Especificar el nombre de las columnas a mantener\ncolumnas_adicionales = [\n    'EMPLID', 'EDAD', 'GENERO', 'CARRERA', 'UNIDAD', 'BACH_PROMEDIO', 'PUNTAJE_ADM',\n    'P005_O001V', 'P005_O002V', 'P006_O001V', 'P006_O002V', 'P006_O003V', 'P006_O004V', 'P006_O005V',\n    'P006_O006V', 'P006_O007V', 'P006_O008V', 'P006_O009V', 'P007_O001V', 'P008_O001V', 'P008_O002V',\n    'P009_O001V', 'P010_O001V', 'P010_O002V', 'P011_O001V', 'P012_O001V', 'P013_O001V', 'P014_O001V',\n    'P015_O001V', 'P015_O002V', 'P015_O003V', 'P015_O004V', 'P015_O005V', 'P016_O001V', 'P017_O001V', \n    'P018_O001V', 'P018_O002V', 'P019_O001V', 'P020_O001V', 'P021_O001V', 'P022_O001V', 'P023_O001V', \n    'P024_O001V', 'P024_O002V', 'P025_O001V', 'P026_O001V', 'P027_O001V', 'P027_O002V', 'P027_O003V', \n    'P027_O004V', 'P027_O005V', 'P027_O006V', 'P027_O007V', 'P027_O008V', 'P027_O009V', 'P027_O010V', \n    'P027_O011V', 'P027_O012V', 'P027_O013V', 'P027_O014V', 'P028_O001V', 'P029_O001V', 'P030_O001V', \n    'P031_O001V', 'P031_O002V', 'P031_O003V', 'P032_O001V', 'P032_O002V', 'P032_O003V', 'P033_O001V', \n    'P033_O002V', 'P033_O003V', 'P033_O004V', 'P033_O005V', 'P033_O006V', 'P033_O007V', 'P033_O008V', \n    'P033_O009V', 'P033_O010V', 'P034_O001V', 'P034_O002V', 'P034_O003V', 'P034_O004V', 'P034_O005V', \n    'P034_O006V', 'P034_O007V', 'P034_O008V', 'P034_O009V', 'P035_O001V', 'P035_O002V', 'P035_O003V', \n    'P035_O004V', 'P035_O005V', 'P036_O001V', 'P036_O002V', 'P036_O003V', 'P036_O004V', 'P036_O005V', \n    'P036_O006V', 'P036_O007V', 'P037_O001V', 'P037_O002V', 'P037_O003V', 'P037_O004V', 'P037_O005V', \n    'P037_O006V', 'P037_O007V', 'P037_O008V', 'P038_O001V', 'P039_O001V', 'P040_O001V', 'P040_O002V', \n    'P041_O001V', 'P041_O002V', 'P041_O003V', 'P041_O004V', 'P041_O005V', 'P041_O006V', 'P042_O001V', \n    'P043_O001V', 'P044_O001V', 'P045_O001V', 'P046_O001V', 'P046_O002V', 'P046_O003V', 'P046_O004V', \n    'P046_O005V', 'P046_O006V', 'P046_O007V', 'P046_O008V', 'P046_O009V', 'P046_O010V', 'P047_O001V', \n    'P047_O002V', 'P047_O003V', 'P047_O004V', 'P047_O005V', 'P047_O006V', 'P047_O007V', 'P047_O008V', \n    'P048_O001V', 'P048_O002V', 'P048_O003V', 'P048_O004V', 'P048_O005V', 'P048_O006V', 'P049_O001V', \n    'P049_O002V', 'P049_O003V', 'P049_O004V', 'P049_O005V', 'P049_O006V', 'P049_O007V', 'P049_O008V', \n    'P049_O009V', 'P049_O010V', 'P049_O011V', 'P050_O001V', 'P050_O002V', 'P050_O003V', 'P050_O004V', \n    'P050_O005V', 'P050_O006V', 'P050_O007V', 'P050_O008V', 'P050_O009V', 'P050_O010V', 'P050_O011V', \n    'P051_O001V', 'P051_O002V', 'P051_O003V', 'P051_O004V', 'P051_O005V', 'P052_O001V', 'P052_O002V', \n    'P052_O003V', 'P052_O004V', 'P052_O005V', 'P052_O006V', 'P052_O007V', 'P052_O008V', 'P052_O009V', \n    'P052_O010V', 'P053_O001V', 'P054_O001V', 'P055_O001V', 'P055_O002V', 'P055_O003V', 'P055_O004V', \n    'P055_O005V', 'P056_O001V', 'P056_O002V', 'P056_O003V', 'P056_O004V', 'P056_O005V', 'P056_O006V', \n    'P056_O007V', 'P056_O008V', 'P056_O009V', 'P056_O010V', 'P057_O001V', 'P058_O001V', 'P059_O001V', \n    'P059_O002V', 'P060_O001V', 'P060_O002V', 'P003_O001V', 'P004_O001V'\n]\n\n# Crear una lista para almacenar los dataframes\ndata_frames = []\n\n# Leer y almacenar cada archivo en la lista de dataframes\nfor file in file_list:\n    file_path = os.path.join(folder_path, file)\n    year = extract_year(file)\n    df = pd.read_csv(file_path, low_memory=False)\n    df['Año'] = year  # Agregar la columna del año\n    \n    # Mantener solo las columnas especificadas\n    columnas_a_mantener = columnas_adicionales + ['Año']\n    \n    # Verificar qué columnas existen realmente en el DataFrame y filtrar\n    columnas_presentes = [col for col in columnas_a_mantener if col in df.columns]\n    df = df[columnas_presentes]\n    \n    data_frames.append(df)\n    \n# Unir todos los dataframes en uno solo\ndf = pd.concat(data_frames, ignore_index=True)\n\n# Eliminar duplicados y reverificar el resultado:\nfilas_antes = df.shape[0]  # Guardar el número de filas antes de eliminar duplicados\ndf = df.drop_duplicates()   # Eliminar filas  duplicadas\nfilas_despues = df.shape[0]   # Guardar el número de filas después\neliminadas = filas_antes - filas_despues # Calcular  cuántas filas se eliminaron\nprint(f\"Se eliminaron {eliminadas} filas duplicadas (de {filas_antes} totales).\")\nprint(f\"DataFrame final: {filas_despues} filas.\")","block_group":"399a8bb2a43044178e1b230b0371a27c","execution_count":2,"outputs":[{"name":"stdout","text":"Se eliminaron 127 filas duplicadas (de 39059 totales).\nDataFrame final: 38932 filas.\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/ce5f6477-d361-43bc-a790-25b4c0c4fb89","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"865fd08","execution_start":1752123003147,"execution_millis":1303,"execution_context_id":"b4734e89-24fa-421e-a465-f05d90503f39","cell_id":"2b2bc7a3b2cd4bb2a2f5eb9d0e59aa82","deepnote_cell_type":"code"},"source":"# 2) COMPLETAR EL DATAFRAME CON LOS DATOS DE DESERCIÓN\n\n#  cargar el archivo de deserción\ndf_desercion = pd.read_excel(os.path.join(folder_path, 'Deserción.xlsx'))\n\n# Convertir a string para asegurar coincidencias\ndf['EMPLID'] = df['EMPLID'].astype(str) \ndf_desercion['EMPLID'] = df_desercion['EMPLID'].astype(str) \n\n# Copiar el df antes del merge (para identificar después quiénes no respondieron)\ndf_original = df.copy()\n\n# Realizar la unión con los datos de deserción por la clave 'EMPLID'\ndf = pd.merge(df, df_desercion, on='EMPLID', how='left')\n\n# Imputar valor 0 en los NaN del campo 'DESERCION' (alumnos que no desertaron pero  respodieron el cuestionario)\ndf['DESERCION'] = df['DESERCION'].fillna(0)\n\n# Verificación de valores nulos en 'DESERCION'\nna_count = df['DESERCION'].isna().sum()\nprint(f\"Valores nulos en 'DESERCION' después de la imputación: {na_count}\")\n\n# Eliminar duplicados que surgieron tras el merge\nfilas_antes_merge = df.shape[0]\ndf = df.drop_duplicates()\nfilas_despues_merge = df.shape[0]\nprint(f\"Se eliminaron {filas_antes_merge - filas_despues_merge} filas duplicadas tras el merge (se generaron durante la unión).\")\nprint(f\"DataFrame final post-merge: {filas_despues_merge} filas.\")\n\n# Ver EMPLIDs que aparecen más de una vez después del merge\nduplicados = df['EMPLID'].value_counts()\nduplicados = duplicados[duplicados > 1]\nprint(f\"Total de duplicados por EMPLID: {duplicados.sum() - len(duplicados)}\")\n\n# Eliminar  registros repetidos (con 'EMPLID' y 'SEMESTRE ABANDONO')\ndf['SEMESTRE ABANDONO'] = pd.to_numeric(df['SEMESTRE ABANDONO'], errors='coerce') # Asegurar que SEMESTRE ABANDONO sea numérico\nduplicados = df[df.duplicated(subset='EMPLID', keep=False)] # Detectar duplicados por EMPLID\nprint(f\"Guardados {duplicados.shape[0]} registros duplicados por EMPLID con posibles abandonos múltiples.\")\n# Quedarse solo con la fila que tenga el mayor 'SEMESTRE ABANDONO' por EMPLID\ndf = df.sort_values(by='SEMESTRE ABANDONO', ascending=False)  # Mayor primero\ndf = df.drop_duplicates(subset='EMPLID', keep='first')  # Conservar solo el mayor por EMPLID\n\n# Confirmar resultado\nprint(f\"DataFrame final depurado: {df.shape[0]} filas (una por EMPLID con abandono máximo)\")\nprint(f\"EMPLIDs únicos: {df['EMPLID'].nunique()}\")","block_group":"db42b3bddc82490a98e552de7ad3533a","execution_count":3,"outputs":[{"name":"stdout","text":"Valores nulos en 'DESERCION' después de la imputación: 0\nSe eliminaron 3 filas duplicadas tras el merge (se generaron durante la unión).\nDataFrame final post-merge: 38945 filas.\nTotal de duplicados por EMPLID: 13\nGuardados 26 registros duplicados por EMPLID con posibles abandonos múltiples.\nDataFrame final depurado: 38932 filas (una por EMPLID con abandono máximo)\nEMPLIDs únicos: 38932\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/dd62546f-3d34-46e0-ab29-c570e0a3bc41","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"1f6fee6","execution_start":1752123004517,"execution_millis":845,"execution_context_id":"b4734e89-24fa-421e-a465-f05d90503f39","cell_id":"387063b426aa41408413600d5eaa0a41","deepnote_cell_type":"code"},"source":"# 3) COMPLETAR EL DATAFRAME CON LOS DATOS DE CAMBIOS DE PROGRAMA\n\n# Cargar tabla de cambios de programa\ncambios_programa = pd.read_excel(os.path.join(folder_path, 'Cambio_Programa.xlsx'))\n\n# Asegurar que los EMPLID sean strings\ndf['EMPLID'] = df['EMPLID'].astype(str)\ncambios_programa['EMPLID'] = cambios_programa['EMPLID'].astype(str)\n\n# Contar número de cambios por EMPLID\nconteo_cambios = cambios_programa['EMPLID'].value_counts().to_dict()\ndf['CAMBIO_PROGRAMA'] = df['EMPLID'].map(conteo_cambios).fillna(0).astype(int)\n\n# Conservar solo la fila con el último cambio de programa por estudiante\n# Suponiendo que SEMESTRE ABANDONO indica el orden temporal\ncambios_programa['SEMESTRE ABANDONO'] = pd.to_numeric(cambios_programa['SEMESTRE ABANDONO'], errors='coerce')\ncambios_unicos = cambios_programa.sort_values('SEMESTRE ABANDONO', ascending=False).drop_duplicates('EMPLID')\n\n# Merge con los campos relevantes desde la tabla depurada\ndf = df.merge(\n    cambios_unicos[['EMPLID', 'SEMESTRE ABANDONO', 'SEMESTRE PLAN', 'ETNIA']],\n    on='EMPLID',\n    how='left',\n    suffixes=('', '_nueva')\n)\n\n# Si ya tenías estos campos en df, completar nulos con los nuevos valores\nfor col in ['SEMESTRE ABANDONO', 'SEMESTRE PLAN', 'ETNIA']:\n    df[col] = df[col].combine_first(df[f\"{col}_nueva\"])\n\n# Eliminar columnas auxiliares\ndf.drop(columns=[f\"{col}_nueva\" for col in ['SEMESTRE ABANDONO', 'SEMESTRE PLAN', 'ETNIA']], inplace=True)\n\n# Eliminar filas duplicadas completas\nfilas_antes = df.shape[0]\ndf = df.drop_duplicates()\nfilas_despues = df.shape[0]\nprint(f\"Se eliminaron {filas_antes - filas_despues} filas duplicadas (de {filas_antes} totales).\")\nprint(f\"DataFrame final: {filas_despues} filas.\")\n","block_group":"4d4f22a2fd12442ea1da2ec8133b6224","execution_count":4,"outputs":[{"name":"stdout","text":"Se eliminaron 0 filas duplicadas (de 38932 totales).\nDataFrame final: 38932 filas.\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/2dfbad9f-73fb-43e4-b1da-b1b4578d93a7","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"4f1da32c","execution_start":1752123005407,"execution_millis":6,"execution_context_id":"b4734e89-24fa-421e-a465-f05d90503f39","cell_id":"23806d88000e4e72ac4826c4b2a2c219","deepnote_cell_type":"code"},"source":"# 3) RESCARTAR DATOS DE UNA COLUMNA (TELEFONO)\n\n# 1. Asignar lada 644 a los teléfonos que inician con 4, asumiento que son de 7 digitos y les falta la lada\n\n# Asegurarse de que la columna de teléfono esté en formato de texto, extraer la lada del número de teléfono y crear columna 'Lada'\ndf['P003_O001V'] = df['P003_O001V'].astype(str).str.replace(r'\\D', '', regex=True) #Aseguramos que la columna P003_O001V contenga solo caracteres numéricos mediante str.replace(r'\\D', '', regex=True), eliminando cualquier carácter no numérico.\ndf['Lada'] = df['P003_O001V'].str[:3]\n\n# Filtrar los números de celular que empiezan con los prefijos especificados\nprefijos = ['410', '411', '412', '413', '414', '415', '416', '417', '418', '419']\nmask_prefijos = df['P003_O001V'].str.startswith(tuple(prefijos))\n\n# Asignar la lada 644 a los números filtrados\ndf.loc[mask_prefijos, 'Lada'] = '644'\n\n# Verificar cuántas filas fueron afectadas por la actualización\nfilas_afectadas = df[mask_prefijos].shape[0]\nprint(f\"Filas afectadas con Lada '644': {filas_afectadas}\")","block_group":"a5535eddd9eb4ed8adff33169280e0be","execution_count":5,"outputs":[{"name":"stdout","text":"Filas afectadas con Lada '644': 256\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/4c37b70a-0540-448e-8b82-ab10abb4d27a","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"a1751fde","execution_start":1752123005467,"execution_millis":0,"execution_context_id":"b4734e89-24fa-421e-a465-f05d90503f39","cell_id":"bdcef9ac95c54a4fae743b364e506024","deepnote_cell_type":"code"},"source":"# 2. Asignar lada 644 a los teléfonos que inician con 044, asumiento que son de la ciudad  y usaban 13 digitos\n\n# Filtrar los números que comienzan con '044'\nmask_044 = df['P003_O001V'].str.startswith('044')\n\n# Asignar la lada 644 a estos números\ndf.loc[mask_044, 'Lada'] = '644'\n\n# Eliminar el prefijo '044' y quedarte con los 10 dígitos restantes del número de teléfono\ndf.loc[mask_044, 'P003_O001V'] = df.loc[mask_044, 'P003_O001V'].str[3:]\n\n# Verificar cuántas filas fueron afectadas por la actualización\nfilas_afectadas = df[mask_044].shape[0]\nprint(f\"Filas afectadas con Lada '644' y modificación de número: {filas_afectadas}\")","block_group":"7bb50d521ef943a69f50fd6937caaf27","execution_count":6,"outputs":[{"name":"stdout","text":"Filas afectadas con Lada '644' y modificación de número: 182\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/6ef48872-072e-4f74-a8a0-531121e4bb1f","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"bbc8d979","execution_start":1752123005527,"execution_millis":105,"execution_context_id":"b4734e89-24fa-421e-a465-f05d90503f39","cell_id":"102fb555371441379ec575dfb97f0145","deepnote_cell_type":"code"},"source":"# 3. Asignar lada 644 a los teléfonos que inician con 1 pero y dicen el nombre del Estado en el domicilio, asumiendo que son de 5 digitos y les falta la lada\n\n# Filtrar los números de teléfono que empiezan con '1'\nmask_prefijo = df['P003_O001V'].str.startswith('1')\n\n# Definir las variaciones del estado \"Sonora\"\nestados = ['Son.', 'Son', 'son', 'son.', 'Sonora', 'sonora']\n\n# Filtrar las filas que contienen variaciones del estado \"Sonora\"\nmask_estados = df['P004_O001V'].str.contains('|'.join(estados), case=False, na=False)\n\n# Aplicar ambas condiciones simultáneamente\ndf.loc[mask_prefijo & mask_estados, 'Lada'] = '644'\n\n# Verificar cuántas filas fueron afectadas por la actualización\nfilas_afectadas = df[mask_prefijo & mask_estados].shape[0]\nprint(f\"Filas afectadas con Lada '644': {filas_afectadas}\")","block_group":"f7e2ffe7b92a41158c139634a4bf0a01","execution_count":7,"outputs":[{"name":"stdout","text":"Filas afectadas con Lada '644': 11\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/97367106-63f3-46c7-b57a-2799f7a42938","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"216af35f","execution_start":1752123005697,"execution_millis":0,"execution_context_id":"b4734e89-24fa-421e-a465-f05d90503f39","cell_id":"7a497bfe5e4140c6b9f345ad6a2e731d","deepnote_cell_type":"code"},"source":"# 4. Asignar lada 644 a los teléfonos que inician con 01, para extraer la lada\n\n# Filtrar los números de celular que empiezan con '01'\nmask_01 = df['P003_O001V'].str.startswith('01')\n\n# Asignar la lada 644 a estos números\ndf.loc[mask_01, 'Lada'] = '644'\n\n# Extraer los tres dígitos siguientes a '01' para obtener la lada real y sobrescribir la columna 'Lada'\ndf.loc[mask_01, 'Lada'] = df.loc[mask_01, 'P003_O001V'].str[2:5]  # Extrae dígitos 2, 3 y 4\n\n# Verificar cuántas filas fueron afectadas\nfilas_afectadas = df[mask_01].shape[0]\nprint(f\"Filas afectadas con Lada actualizada: {filas_afectadas}\")","block_group":"b0215656d6044022a75ec357d6735cd4","execution_count":8,"outputs":[{"name":"stdout","text":"Filas afectadas con Lada actualizada: 19\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/26885461-503a-4849-84b6-c4b46de20a9a","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"43577d2e","execution_start":1752123005757,"execution_millis":0,"execution_context_id":"b4734e89-24fa-421e-a465-f05d90503f39","cell_id":"783561b7419e46798117c7145eb545ea","deepnote_cell_type":"code"},"source":"# 5. Asignar lada 644 a los teléfonos que inician con 1 y tienen más de 7 digitos, para extraer la lada\n\n# Filtrar los números de celular que empiezan con '1' y tienen más de 7 dígitos\nmask_1_y_largo = df['P003_O001V'].str.startswith('1') & (df['P003_O001V'].str.len() > 7)\n\n# Asignar la lada 644 a estos números inicialmente\ndf.loc[mask_1_y_largo, 'Lada'] = '644'\n\n# Extraer los tres dígitos siguientes al primer dígito '1' para obtener la lada real y actualizar la columna 'Lada'\ndf.loc[mask_1_y_largo, 'Lada'] = df.loc[mask_1_y_largo, 'P003_O001V'].str[1:4]  # Extrae dígitos 2, 3 y 4\n\n# Verificar cuántas filas fueron afectadas\nfilas_afectadas = df[mask_1_y_largo].shape[0]\nprint(f\"Filas afectadas con Lada actualizada: {filas_afectadas}\")","block_group":"f3e6efd3eafd46138ffb3d42d0728920","execution_count":9,"outputs":[{"name":"stdout","text":"Filas afectadas con Lada actualizada: 28\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/e23172b3-8ec0-4c4d-a593-78fc1ab38261","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"1341b1d0","execution_start":1752123005807,"execution_millis":0,"execution_context_id":"b4734e89-24fa-421e-a465-f05d90503f39","cell_id":"f7f33b7686144a8b8e801fb94d9d4929","deepnote_cell_type":"code"},"source":"# 6. Asignar lada 644 a los teléfonos que inician con 1\n\n# Filtrar los números de celular que empiezan con '1'\nmask_1 = df['P003_O001V'].str.startswith('1')\n\n# Asignar la lada 644 a estos números\ndf.loc[mask_1, 'Lada'] = '644'\n\n# Verificar cuántas filas fueron afectadas\nfilas_afectadas = df[mask_1].shape[0]\nprint(f\"Filas afectadas con Lada actualizada: {filas_afectadas}\")","block_group":"60f8b2c156614709b38e6ad339a83c65","execution_count":10,"outputs":[{"name":"stdout","text":"Filas afectadas con Lada actualizada: 142\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/514eafbb-bf77-4ab0-85f1-de3a6baf3bd7","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"262f6fa9","execution_start":1752123005867,"execution_millis":0,"execution_context_id":"b4734e89-24fa-421e-a465-f05d90503f39","cell_id":"ec67cc01f43744a0af08bc4f43637bc6","deepnote_cell_type":"code"},"source":"# 7. Asignar lada 622 a los teléfonos que inician con 22\n\n# Filtrar los números de celular que empiezan con '22'\nmask_22 = df['P003_O001V'].str.startswith('22')\n\n# Asignar la lada 644 a estos números\ndf.loc[mask_22, 'Lada'] = '622'\n\n# Verificar cuántas filas fueron afectadas por la actualización\nfilas_afectadas = df[mask_22].shape[0]\nprint(f\"Filas afectadas con Lada '622': {filas_afectadas}\")","block_group":"2fc51c99a004402ead92d27d383d6627","execution_count":11,"outputs":[{"name":"stdout","text":"Filas afectadas con Lada '622': 114\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/1c7e3c22-19d8-4e4d-9d17-f6775aa31070","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"71dae745","execution_start":1752123005917,"execution_millis":119,"execution_context_id":"b4734e89-24fa-421e-a465-f05d90503f39","cell_id":"53a81d8aef4c404e862cec3385e17ed7","deepnote_cell_type":"code"},"source":"# COMBINAR el df con la tabla de dimensiones 'lada_to_location' para obtener campos de Ciudad y Estado\n\n# Cargar el archivo de mapeo de ladas a ubicaciones con codificación especificada\ntry:\n    with open(os.path.join(folder_path, 'lada_to_location.csv'), 'r', encoding='utf-8-sig') as file:\n        lada_to_location = pd.read_csv(file)\nexcept UnicodeDecodeError:\n    try:\n        lada_to_location = pd.read_csv(os.path.join(folder_path, 'lada_to_location.csv'), encoding='ISO-8859-1')\n    except UnicodeDecodeError:\n        try:\n            lada_to_location = pd.read_csv(os.path.join(folder_path, 'lada_to_location.csv'), encoding='Windows-1252')\n        except UnicodeDecodeError:\n            print(\"Error al leer el archivo CSV con codificaciones comunes. Verifica el archivo y la codificación.\")\n\n# Asegurarse de que la columna 'Lada' en lada_to_location  esté en formato de texto\nlada_to_location['Lada'] = lada_to_location['Lada'].astype(str)\n\n# Unir el DataFrame principal con el DataFrame de mapeo por la columna 'Lada'\ndf = df.merge(lada_to_location, on='Lada', how='left')  \n\n# Verificar duplicados por EMPLID generados después del merge\nemplids_antes = df['EMPLID'].nunique()\nfilas_antes = df.shape[0]\n\n# Eliminar duplicados por EMPLID (conservando la primera ocurrencia)\ndf = df.drop_duplicates(subset='EMPLID', keep='first')\n\n# Confirmar cambio\nfilas_despues = df.shape[0]\nprint(f\"Se eliminaron {filas_antes - filas_despues} filas duplicadas por EMPLID generadas después del merge.\")\nprint(f\"DataFrame final con 1 fila por estudiante: {df.shape[0]} filas, {df['EMPLID'].nunique()} EMPLIDs únicos.\")\n\n# Verificación posterior al merge\ncampos_nuevos = ['Ciudad', 'Estado']\ncampos_existentes = [col for col in campos_nuevos if col in df.columns]\n\nif campos_existentes:\n    filas_completas = df[campos_existentes].notnull().all(axis=1).sum()\n    filas_incompletas = df.shape[0] - filas_completas\n    print(f\"Merge completado: se asignaron Ciudad y Estado en {filas_completas} de {df.shape[0]} registros.\")\n    print(f\"{filas_incompletas} registros tienen valores nulos en ambos campos ('Ciudad' y 'Estado').\")\nelse:\n    print(\"Atención: No se encontraron columnas 'Ciudad' y 'Estado' tras el merge.\")","block_group":"f8bb6afd54a94220b802bdfd6806d6fe","execution_count":12,"outputs":[{"name":"stdout","text":"Se eliminaron 891 filas duplicadas por EMPLID generadas después del merge.\nDataFrame final con 1 fila por estudiante: 38932 filas, 38932 EMPLIDs únicos.\nMerge completado: se asignaron Ciudad y Estado en 38224 de 38932 registros.\n708 registros tienen valores nulos en ambos campos ('Ciudad' y 'Estado').\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/04857d2e-a0ee-4579-ad0f-b67cc974d7a9","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"9eb47e61","execution_start":1752123006087,"execution_millis":4767,"execution_context_id":"b4734e89-24fa-421e-a465-f05d90503f39","cell_id":"6a5bdb602909402094b221aeec2b8b1b","deepnote_cell_type":"code"},"source":"# COMPLEMENTAR REGISTROS VACIOS EN CIUDAD/ESTADO CON DATOS DEL DOMICILIO\n\nfrom unidecode import unidecode\n\n# Función para buscar coincidencias parciales en el domicilio y actualizar las columnas correspondientes\ndef actualizar_por_domicilio(df, lada_to_location):\n    # Normalizar el DataFrame 'lada_to_location' quitando acentos y poniendo en minúsculas\n    lada_to_location['Ciudad_Normalizada'] = lada_to_location['Ciudad'].apply(lambda x: unidecode(str(x)).lower())\n    lada_to_location['Estado_Normalizado'] = lada_to_location['Estado'].apply(lambda x: unidecode(str(x)).lower())\n    \n    # Normalizar las columnas relevantes del DataFrame df\n    df['P004_O001V_Normalizado'] = df['P004_O001V'].apply(lambda x: unidecode(str(x)).lower())\n    \n    # Conjunto de estados válidos\n    estados_validos = set(lada_to_location['Estado_Normalizado'].unique())\n    \n    # Lista de estados que deben ser excluidos (para evitar que entienda que el estado es Colima porque solo se refiere a la colonia)\n    estados_excluidos = {'Col.', 'col.', 'colonia', 'colon', 'col', 'colim', 'colim.', 'co.'}\n\n    # Recorrer el DataFrame original en busca de coincidencias\n    for i, domicilio_normalizado in df['P004_O001V_Normalizado'].items():\n        if pd.isna(df.loc[i, 'Lada']) or pd.isna(df.loc[i, 'Ciudad']) or pd.isna(df.loc[i, 'Estado']):\n            # Normalizar el domicilio (quitar acentos y pasar a minúsculas)\n            domicilio_normalizado = unidecode(str(domicilio_normalizado)).lower()\n            \n            # Leer el domicilio de derecha a izquierda\n            palabras_domicilio = domicilio_normalizado.split()[::-1]  # Dividir en palabras e invertir el orden\n            \n            # Variable para rastrear si se encontró alguna coincidencia\n            encontrado = False\n            \n            # Buscar coincidencias en las ciudades y estados\n            for _, row in lada_to_location.iterrows():\n                ciudad_normalizada = row['Ciudad_Normalizada']\n                estado_normalizado = row['Estado_Normalizado']\n                \n                # Verificar si alguna palabra del domicilio coincide parcialmente con la ciudad\n                for palabra in palabras_domicilio:\n                    if ciudad_normalizada in palabra:\n                        df.loc[i, 'Ciudad'] = row['Ciudad']\n                        df.loc[i, 'Estado'] = row['Estado']\n                        df.loc[i, 'Lada'] = row['Lada']\n                        encontrado = True\n                        break\n                \n                # Verificar si alguna palabra coincide parcialmente con el estado si no se encontró ciudad\n                if not encontrado:\n                    for palabra in palabras_domicilio:\n                        if estado_normalizado in palabra and estado_normalizado not in estados_excluidos:\n                            if estado_normalizado in estados_validos:\n                                df.loc[i, 'Estado'] = row['Estado']\n                                if pd.isna(df.loc[i, 'Lada']):\n                                    df.loc[i, 'Lada'] = row['Lada']\n                            break\n\n            # Si no se encontró ciudad o estado, usar el valor de la columna 'UNIDAD'\n            if pd.isna(df.loc[i, 'Ciudad']):\n                unidad = df.loc[i, 'UNIDAD']\n                if unidad == 'OBREGON':\n                    df.loc[i, 'Ciudad'] = \"Ciudad Obregón\"\n                elif unidad == 'NAVOJOA':\n                    df.loc[i, 'Ciudad'] = \"Navojoa\"\n                elif unidad == 'GUAYMAS':\n                    df.loc[i, 'Ciudad'] = \"Guaymas\"\n                elif unidad == 'EMPALME':\n                    df.loc[i, 'Ciudad'] = \"Empalme\"\n                else:\n                    df.loc[i, 'Ciudad'] = None  # Si no es ninguno de estos, deja NaN\n\n            # Si no se encontró estado, usar el valor de la columna 'UNIDAD'\n            if pd.isna(df.loc[i, 'Estado']):\n                unidad = df.loc[i, 'UNIDAD']\n                if unidad == 'OBREGON':\n                    df.loc[i, 'Estado'] = \"Sonora\"\n                elif unidad == 'NAVOJOA':\n                    df.loc[i, 'Estado'] = \"Sonora\"\n                elif unidad == 'GUAYMAS':\n                    df.loc[i, 'Estado'] = \"Sonora\"\n                elif unidad == 'EMPALME':\n                    df.loc[i, 'Estado'] = \"Sonora\"\n                else:\n                    df.loc[i, 'Estado'] = None  # Si no es ninguno de estos, deja NaN\n\n    # Eliminar columna de normalización\n    df.drop(columns=['P004_O001V_Normalizado'], inplace=True)\n\n    return df\n\n# Contar las filas con NaN en 'Ciudad', 'Estado' y 'Lada' antes de la actualización\nfilas_con_nan_antes = {\n    'Ciudad': df['Ciudad'].isna().sum(),\n    'Estado': df['Estado'].isna().sum(),\n}\n\n# Llamar a la función para actualizar el DataFrame df\ndf = actualizar_por_domicilio(df, lada_to_location)\n\n# Contar las filas con valores NaN en 'Ciudad', 'Estado' y 'Lada' después de la actualización\nfilas_con_nan_despues = {\n    'Ciudad': df['Ciudad'].isna().sum(),\n    'Estado': df['Estado'].isna().sum(),\n}\n\n# Calcular la diferencia\nfilas_modificadas = {\n    'Ciudad': filas_con_nan_antes['Ciudad'] - filas_con_nan_despues['Ciudad'],\n    'Estado': filas_con_nan_antes['Estado'] - filas_con_nan_despues['Estado'],\n}\n\nprint(f\"Filas con 'Ciudad' NaN antes: {filas_con_nan_antes['Ciudad']}\")\nprint(f\"Filas con 'Ciudad' NaN después: {filas_con_nan_despues['Ciudad']}\")\nprint(f\"Filas actualizadas en 'Ciudad': {filas_modificadas['Ciudad']}\")\n\nprint(f\"Filas con 'Estado' NaN antes: {filas_con_nan_antes['Estado']}\")\nprint(f\"Filas con 'Estado' NaN después: {filas_con_nan_despues['Estado']}\")\nprint(f\"Filas actualizadas en 'Estado': {filas_modificadas['Estado']}\")","block_group":"e48026d961db4cd5b082a27599eca212","execution_count":13,"outputs":[{"name":"stdout","text":"Filas con 'Ciudad' NaN antes: 708\nFilas con 'Ciudad' NaN después: 0\nFilas actualizadas en 'Ciudad': 708\nFilas con 'Estado' NaN antes: 708\nFilas con 'Estado' NaN después: 0\nFilas actualizadas en 'Estado': 708\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/bfb4a0b7-57c5-4447-b244-b1a058c5258d","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"38370de5","execution_start":1752123010917,"execution_millis":144442,"execution_context_id":"b4734e89-24fa-421e-a465-f05d90503f39","cell_id":"da3b6cbec56446c5a07bf8e81a81d77b","deepnote_cell_type":"code"},"source":"# OBTENER COORDENADAS\n\nfrom geopy.geocoders import Nominatim\nfrom geopy.exc import GeocoderTimedOut, GeocoderUnavailable\nfrom geopy.distance import geodesic\nimport time\n\n# Inicializar el geolocalizador con tiempo de espera más largo y retries\ngeolocator = Nominatim(user_agent=\"geoapi\", timeout=10)\n\n# Función para obtener coordenadas con manejo de errores y reintentos\ndef obtener_coordenadas(ciudad, max_intentos=3):\n    for intento in range(max_intentos):\n        try:\n            location = geolocator.geocode(f\"{ciudad}, México\")\n            if location:\n                return location.latitude, location.longitude\n            else:\n                return None, None\n        except (GeocoderTimedOut, GeocoderUnavailable) as e:\n            if intento == max_intentos - 1:  # Si es el último intento\n                print(f\"Error: No se pudo obtener coordenadas para {ciudad} después de {max_intentos} intentos\")\n                return None, None\n            print(f\"Intento {intento + 1} falló para {ciudad}. Reintentando...\")\n            time.sleep(2)  # Esperar 2 segundos antes de reintentar\n\n# Extraer las ciudades únicas del DataFrame\ncolumna_ciudad = \"Ciudad\"\nciudades_unicas = df[columna_ciudad].dropna().unique()\n\n# Crear un diccionario para almacenar las coordenadas y evitar buscar repetidamente\ncoordenadas_dict = {}\n\n# Procesar cada ciudad con reintento y pausa entre solicitudes\nfor ciudad in ciudades_unicas:\n    print(f\"Obteniendo coordenadas para: {ciudad}\")\n    coordenadas_dict[ciudad] = obtener_coordenadas(ciudad)\n    time.sleep(1)  # Pausa de 1 segundo entre solicitudes para evitar sobrecarga\n\n# Crear las nuevas columnas de Latitud y Longitud en el DataFrame original\ndf['Latitud'] = df[columna_ciudad].map(lambda x: coordenadas_dict[x][0] if x in coordenadas_dict else None)\ndf['Longitud'] = df[columna_ciudad].map(lambda x: coordenadas_dict[x][1] if x in coordenadas_dict else None)\n\n# Verificar si hay ciudades sin coordenadas\nciudades_sin_coordenadas = df[df['Latitud'].isnull()]\nif not ciudades_sin_coordenadas.empty:\n    print(\"\\nNo se encontraron coordenadas para las siguientes ciudades:\")\n    print(ciudades_sin_coordenadas[columna_ciudad].unique())\nelse:\n    print(\"\\nTodas las ciudades tienen coordenadas.\")","block_group":"c930a5c41f774d129fbd1a0777e28049","execution_count":14,"outputs":[{"name":"stdout","text":"Obteniendo coordenadas para: Ciudad Obregón\nObteniendo coordenadas para: Guaymas\nObteniendo coordenadas para: San Ignacio\nObteniendo coordenadas para: Vicam\nObteniendo coordenadas para: Temoris\nObteniendo coordenadas para: Navojoa\nObteniendo coordenadas para: Huatabampo\nObteniendo coordenadas para: Guasave\nObteniendo coordenadas para: Agua Prieta\nObteniendo coordenadas para: Tijuana\nObteniendo coordenadas para: Nogales\nObteniendo coordenadas para: Hermosillo\nObteniendo coordenadas para: Acapulco\nObteniendo coordenadas para: Los Mochis\nObteniendo coordenadas para: Nacozari\nObteniendo coordenadas para: Yécora\nObteniendo coordenadas para: Mazatlán\nObteniendo coordenadas para: Guamuchil\nObteniendo coordenadas para: Angostura\nObteniendo coordenadas para: Caborca\nObteniendo coordenadas para: El Fuerte\nObteniendo coordenadas para: Mexicali\nObteniendo coordenadas para: San Luis Potosí\nObteniendo coordenadas para: San Luis Río Colorado\nObteniendo coordenadas para: Manzanillo\nObteniendo coordenadas para: Ensenada\nObteniendo coordenadas para: Puerto Peñasco\nObteniendo coordenadas para: Ciudad Victoria\nObteniendo coordenadas para: La Paz\nObteniendo coordenadas para: Empalme\nObteniendo coordenadas para: Morelia\nObteniendo coordenadas para: Polotitlán\nObteniendo coordenadas para: Toluca\nObteniendo coordenadas para: Guadalajara\nObteniendo coordenadas para: Zapopan\nObteniendo coordenadas para: Ciudad Cuauhtémoc\nObteniendo coordenadas para: Magdalena\nObteniendo coordenadas para: Tuxtla Gutiérrez\nObteniendo coordenadas para: San José del Cabo\nObteniendo coordenadas para: León\nObteniendo coordenadas para: Culiacán\nObteniendo coordenadas para: Jiménez\nObteniendo coordenadas para: Fresnillo\nObteniendo coordenadas para: San Marcos Nepantla\nObteniendo coordenadas para: Oaxaca\nObteniendo coordenadas para: Querétaro\nObteniendo coordenadas para: Irapuato\nObteniendo coordenadas para: Chihuahua\nObteniendo coordenadas para: Torreón\nObteniendo coordenadas para: Moroleón\nObteniendo coordenadas para: Villahermosa\nObteniendo coordenadas para: Puerto Vallarta\nObteniendo coordenadas para: Colima\nObteniendo coordenadas para: Cajeme\nObteniendo coordenadas para: Santa Ana\nObteniendo coordenadas para: México\nObteniendo coordenadas para: Sonoyta\nObteniendo coordenadas para: Cananea\nObteniendo coordenadas para: Zacatecas\nObteniendo coordenadas para: Ciudad Constitución\nObteniendo coordenadas para: Chinameca\nObteniendo coordenadas para: Tampico\nObteniendo coordenadas para: Cuernavaca\nObteniendo coordenadas para: Guanajuato\nObteniendo coordenadas para: Ahome\nObteniendo coordenadas para: Tlaxcala\nObteniendo coordenadas para: Campeche\nObteniendo coordenadas para: Ciudad Juárez\nObteniendo coordenadas para: Tecate\nObteniendo coordenadas para: Tepeji del Rio\nObteniendo coordenadas para: Córdoba\nObteniendo coordenadas para: Ciudad Guzmán\nObteniendo coordenadas para: Tapachula\nObteniendo coordenadas para: Mérida\nObteniendo coordenadas para: Saltillo\nObteniendo coordenadas para: Chilpancingo\nObteniendo coordenadas para: Ixtepec\nObteniendo coordenadas para: Aguascalientes\nObteniendo coordenadas para: Apizaco\nObteniendo coordenadas para: Etchojoa\nObteniendo coordenadas para: Durango\nObteniendo coordenadas para: Matamoros\nObteniendo coordenadas para: Veracruz\nObteniendo coordenadas para: Coatzacoalcos\nObteniendo coordenadas para: Cancún\nObteniendo coordenadas para: Atitalaquia\nObteniendo coordenadas para: Esperanza\nObteniendo coordenadas para: Mayanalán\nObteniendo coordenadas para: Monterrey\nObteniendo coordenadas para: Ciudad Lázaro Cárdenas\nObteniendo coordenadas para: Chetumal\nObteniendo coordenadas para: Salamanca\nObteniendo coordenadas para: Uruapan\nObteniendo coordenadas para: Pachuca\nObteniendo coordenadas para: Texcoco\nObteniendo coordenadas para: Acaponeta\nObteniendo coordenadas para: Cocorit\nObteniendo coordenadas para: Parral\nObteniendo coordenadas para: Ciudad Delicias\nObteniendo coordenadas para: Tobarito\nObteniendo coordenadas para: Zacatepec\nObteniendo coordenadas para: Orizaba\nObteniendo coordenadas para: Tepic\nObteniendo coordenadas para: Reynosa\nObteniendo coordenadas para: San Felipe del Progreso\nObteniendo coordenadas para: Nuevo Laredo\nObteniendo coordenadas para: Cuautla\nObteniendo coordenadas para: La Piedad\nObteniendo coordenadas para: Ciudad Mante\nObteniendo coordenadas para: Celaya\nObteniendo coordenadas para: Monclova\nObteniendo coordenadas para: Puebla\nObteniendo coordenadas para: Tuxpan\nObteniendo coordenadas para: Poza Rica\nObteniendo coordenadas para: Zihuatanejo\n\nTodas las ciudades tienen coordenadas.\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/752f721d-c350-4726-9031-97eb917d1128","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"5f88bfcf","execution_start":1752123155407,"execution_millis":527,"execution_context_id":"b4734e89-24fa-421e-a465-f05d90503f39","cell_id":"7da5aeda30424a61b2a579eeb3e00b3d","deepnote_cell_type":"code"},"source":"# Imputar UNIDAD basándose en Ciudad\n\n# Diccionario de mapeo explícito\nciudad_a_unidad = {\n    'Ciudad Obregón': 'OBREGON',\n    'Navojoa': 'NAVOJOA',\n    'Guaymas': 'GUAYMAS',\n    'Empalme': 'EMPALME'\n}\n\n# Contar cuántos NaN había antes\nnum_nan_unidad_antes = df['UNIDAD'].isna().sum()\n\n# Imputar valores en UNIDAD solo si está nulo y la ciudad está en el diccionario\ndf['UNIDAD'] = df.apply(\n    lambda row: ciudad_a_unidad[row['Ciudad']] if pd.isna(row['UNIDAD']) and row['Ciudad'] in ciudad_a_unidad else row['UNIDAD'],\n    axis=1\n)\n\n# Contar cuántos NaN quedan\nnum_nan_unidad_despues = df['UNIDAD'].isna().sum()\nnum_imputados = num_nan_unidad_antes - num_nan_unidad_despues\n\nprint(f\"Se imputaron {num_imputados} valores faltantes en 'UNIDAD' usando la ciudad como referencia.\")","block_group":"0fbb2c7ac89c4c63bcef031d549ba5c3","execution_count":15,"outputs":[{"name":"stdout","text":"Se imputaron 1 valores faltantes en 'UNIDAD' usando la ciudad como referencia.\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/1a53c3d0-9de9-42f2-b318-0cdc053815a5","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"a199b2bb","execution_start":1752123155997,"execution_millis":6191,"execution_context_id":"b4734e89-24fa-421e-a465-f05d90503f39","cell_id":"26a9018fd6c242e4aa52d657003a9bf7","deepnote_cell_type":"code"},"source":"# CALCULAR DISTANCIA AL CAMPUS:\n\n# Coordenadas de cada campus\ncoordenadas_campus = {\n    \"OBREGON\": (27.49213, -109.97236),\n    \"NAVOJOA\": (27.05561, -109.46021),\n    \"GUAYMAS\": (27.96751, -110.91968),\n    \"EMPALME\": (27.96160, -110.79476)\n}\n\n# Asegurarte de que las columnas 'Ciudad', 'Latitud', 'Longitud' y 'UNIDAD' existan en el DataFrame\nif not all(col in df.columns for col in ['Ciudad', 'Latitud', 'Longitud', 'UNIDAD']):\n    raise ValueError(\"El DataFrame no tiene las columnas necesarias: 'Ciudad', 'Latitud', 'Longitud', y 'UNIDAD'.\")\n\n# Crear un diccionario con coordenadas de ciudades válidas\nciudades_coordenadas = {\n    ciudad: (lat, lon) for ciudad, lat, lon in zip(df['Ciudad'], df['Latitud'], df['Longitud']) \n    if pd.notnull(lat) and pd.notnull(lon)\n}\n\n# Verificar si hay ciudades con coordenadas faltantes\nciudades_sin_coordenadas = set(df['Ciudad']) - set(ciudades_coordenadas.keys())\nif ciudades_sin_coordenadas:\n    print(\"Las siguientes ciudades no tienen coordenadas válidas y no se calculará la distancia:\")\n    print(ciudades_sin_coordenadas)\n\n# Función para calcular la distancia entre la ciudad del estudiante y el campus correspondiente\ndef calcular_distancia(row):\n    ciudad = row['Ciudad']\n    unidad = row['UNIDAD']\n    \n    # Verificar si la ciudad y la unidad tienen coordenadas válidas\n    if ciudad in ciudades_coordenadas and unidad in coordenadas_campus:\n        coords_ciudad = ciudades_coordenadas[ciudad]\n        coords_campus = coordenadas_campus[unidad]\n        return geodesic(coords_ciudad, coords_campus).kilometers\n    else:\n        return None\n\n# Aplicar la función a cada fila del DataFrame\ndf['Distancia_Campus'] = df.apply(calcular_distancia, axis=1)\n\n# Verificar filas con distancias nulas\ndistancias_nulas = df[df['Distancia_Campus'].isnull()]\nif not distancias_nulas.empty:\n    print(\"Algunas filas tienen distancias nulas. Verificar las ciudades y unidades no mapeadas:\")\n    print(distancias_nulas[['Ciudad', 'UNIDAD']].drop_duplicates())\nelse:\n    print(\"Todas las distancias al campus fueron calculadas exitosamente.\")","block_group":"dd7190841ac2467f9097118830557ad9","execution_count":16,"outputs":[{"name":"stdout","text":"Todas las distancias al campus fueron calculadas exitosamente.\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/47c9288b-7ed4-458b-aff8-b2df26b7ba69","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"9a6b5ea1","execution_start":1752123162237,"execution_millis":23,"execution_context_id":"b4734e89-24fa-421e-a465-f05d90503f39","cell_id":"e058535b52874a32b1a2d1b4dc7cea46","deepnote_cell_type":"code"},"source":"# DETECCION DE OUTLIERS EN DISTANCIA CAMPUS E IMPUTACION DE MODA/MEDIANA SEGUN CAMPUS\n\n# Función para imputar valores de outliers\n\ndef imputar_outliers_distancia(df, columna_distancia='Distancia_Campus', columna_unidad='UNIDAD'):\n    # Para cada grupo (campus) en la columna UNIDAD\n    for campus, grupo in df.groupby(columna_unidad):\n        # Calcular el primer y tercer cuartil y el IQR\n        Q1 = grupo[columna_distancia].quantile(0.25)\n        Q3 = grupo[columna_distancia].quantile(0.75)\n        IQR = Q3 - Q1\n        \n        # Definir los límites inferior y superior (usando el factor 1.5)\n        lower_bound = Q1 - 1.5 * IQR\n        upper_bound = Q3 + 1.5 * IQR\n        \n        # Filtrar los valores que NO son outliers para calcular la moda\n        valores_validos = grupo[(grupo[columna_distancia] >= lower_bound) & (grupo[columna_distancia] <= upper_bound)][columna_distancia]\n        \n        if not valores_validos.empty:\n            mode_valores = valores_validos.mode()\n            if not mode_valores.empty:\n                impute_value = mode_valores.iloc[0]\n            else:\n                # Si no se encuentra modo, se usa la mediana\n                impute_value = valores_validos.median()\n        else:\n            impute_value = grupo[columna_distancia].median()\n        \n        # Identificar las filas outlier en este grupo y reemplazarlas\n        mask = (df[columna_unidad] == campus) & ((df[columna_distancia] < lower_bound) | (df[columna_distancia] > upper_bound))\n        df.loc[mask, columna_distancia] = impute_value\n        \n        print(f\"Para la UNIDAD '{campus}':\")\n        print(f\"  Límite inferior: {lower_bound:.2f}, Límite superior: {upper_bound:.2f}\")\n        print(f\"  Valor imputado: {impute_value:.2f}, Outliers imputados: {mask.sum()}\")\n        print(\"-----------------------------------------------------\")\n        \n    return df\n\n# Aplicar la función al DataFrame\ndf = imputar_outliers_distancia(df)\n\n# Mostrar resumen de la variable después de la imputación\nprint(\"Resumen de 'Distancia_Campus' después de la imputación:\")\nprint(df['Distancia_Campus'].describe())","block_group":"1f41689c2d5645cea24ab6c1c3891585","execution_count":17,"outputs":[{"name":"stdout","text":"Para la UNIDAD 'EMPALME':\n  Límite inferior: 11.21, Límite superior: 11.21\n  Valor imputado: 11.21, Outliers imputados: 126\n-----------------------------------------------------\nPara la UNIDAD 'GUAYMAS':\n  Límite inferior: 5.46, Límite superior: 5.46\n  Valor imputado: 5.46, Outliers imputados: 306\n-----------------------------------------------------\nPara la UNIDAD 'NAVOJOA':\n  Límite inferior: -76.14, Límite superior: 133.44\n  Valor imputado: 2.45, Outliers imputados: 347\n-----------------------------------------------------\nPara la UNIDAD 'OBREGON':\n  Límite inferior: -95.28, Límite superior: 169.21\n  Valor imputado: 3.90, Outliers imputados: 4081\n-----------------------------------------------------\nResumen de 'Distancia_Campus' después de la imputación:\ncount    38932.000000\nmean        13.605052\nstd         27.723889\nmin          2.452858\n25%          3.901525\n50%          3.901525\n75%          5.460373\nmax        145.207588\nName: Distancia_Campus, dtype: float64\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/710ef40f-defb-4f1c-94a4-026a525d9d88","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"99ed9ae1","execution_start":1752123162317,"execution_millis":0,"execution_context_id":"b4734e89-24fa-421e-a465-f05d90503f39","cell_id":"b40fcf01fe2d40f18a9696c329b2d7f0","deepnote_cell_type":"code"},"source":"# Eliminar columnas que ya no se necesitan\ncolumnas_a_eliminar = ['Lada', 'P003_O001V', 'P004_O001V']\ndf.drop(columns=columnas_a_eliminar, inplace=True, errors='ignore')\n\n# Verificar que se eliminaron\nprint(f\"Columnas eliminadas: {', '.join(columnas_a_eliminar)}\")\nprint(f\"Columnas restantes en el DataFrame: {df.shape[1]}\")","block_group":"7fb87c002efd4e5c9b8f966b9180a043","execution_count":18,"outputs":[{"name":"stdout","text":"Columnas eliminadas: Lada, P003_O001V, P004_O001V\nColumnas restantes en el DataFrame: 219\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/c9fcc5fe-c237-40f6-bccf-d7a701af7ddd","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"35fb05d8","execution_start":1752123162387,"execution_millis":3,"execution_context_id":"b4734e89-24fa-421e-a465-f05d90503f39","cell_id":"d4218766af98463fb7c63a418ca872e5","deepnote_cell_type":"code"},"source":"# INSPECCION ANTES DE INICIAR CONVERSIÓN DEL TIPO DE DATOS E IMPUTACION DE VALORES\n\n# Obtener una visión general rápida de la estructura del df,  tipo de datos, y el uso de memoria.\ndf.info()\n\n# Obtener listas de columnas por tipo de dato\nfloat64_columns = df.select_dtypes(include='float64').columns\nint64_columns = df.select_dtypes(include='int64').columns\nobject_columns = df.select_dtypes(include='object').columns\n\nprint(f\"Columnas float64 ({len(float64_columns)}):\")\nprint(float64_columns.tolist())\n\nprint(f\"\\nColumnas int64 ({len(int64_columns)}):\")\nprint(int64_columns.tolist())\n\nprint(f\"\\nColumnas object ({len(object_columns)}):\")\nprint(object_columns.tolist())\n","block_group":"3de4e4cc0eff4a4590dff40629238d54","execution_count":19,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nIndex: 38932 entries, 0 to 39822\nColumns: 219 entries, EMPLID to Distancia_Campus\ndtypes: float64(193), int64(9), object(17)\nmemory usage: 66.4+ MB\nColumnas float64 (193):\n['BACH_PROMEDIO', 'P006_O001V', 'P006_O002V', 'P006_O003V', 'P006_O004V', 'P006_O005V', 'P006_O006V', 'P006_O007V', 'P007_O001V', 'P008_O001V', 'P009_O001V', 'P010_O001V', 'P011_O001V', 'P013_O001V', 'P014_O001V', 'P015_O001V', 'P015_O002V', 'P015_O003V', 'P015_O004V', 'P015_O005V', 'P016_O001V', 'P017_O001V', 'P018_O001V', 'P019_O001V', 'P020_O001V', 'P021_O001V', 'P022_O001V', 'P023_O001V', 'P024_O001V', 'P024_O002V', 'P025_O001V', 'P026_O001V', 'P027_O001V', 'P027_O002V', 'P027_O003V', 'P027_O004V', 'P027_O005V', 'P027_O006V', 'P027_O007V', 'P027_O008V', 'P027_O009V', 'P027_O010V', 'P027_O011V', 'P027_O012V', 'P027_O013V', 'P027_O014V', 'P028_O001V', 'P030_O001V', 'P031_O001V', 'P031_O002V', 'P031_O003V', 'P032_O001V', 'P032_O002V', 'P032_O003V', 'P033_O002V', 'P033_O003V', 'P033_O004V', 'P033_O005V', 'P033_O006V', 'P033_O007V', 'P033_O008V', 'P033_O009V', 'P033_O010V', 'P034_O001V', 'P034_O002V', 'P034_O003V', 'P034_O004V', 'P034_O005V', 'P034_O006V', 'P034_O007V', 'P034_O008V', 'P034_O009V', 'P035_O001V', 'P035_O002V', 'P035_O003V', 'P035_O004V', 'P035_O005V', 'P036_O001V', 'P036_O002V', 'P036_O003V', 'P036_O004V', 'P036_O005V', 'P036_O006V', 'P036_O007V', 'P037_O001V', 'P037_O002V', 'P037_O003V', 'P037_O004V', 'P037_O005V', 'P037_O006V', 'P037_O007V', 'P037_O008V', 'P038_O001V', 'P039_O001V', 'P040_O001V', 'P040_O002V', 'P041_O001V', 'P041_O002V', 'P041_O003V', 'P041_O004V', 'P041_O005V', 'P041_O006V', 'P042_O001V', 'P043_O001V', 'P044_O001V', 'P045_O001V', 'P046_O001V', 'P046_O002V', 'P046_O003V', 'P046_O004V', 'P046_O005V', 'P046_O006V', 'P046_O007V', 'P046_O008V', 'P046_O009V', 'P046_O010V', 'P047_O001V', 'P047_O002V', 'P047_O003V', 'P047_O004V', 'P047_O005V', 'P047_O006V', 'P047_O007V', 'P048_O002V', 'P048_O003V', 'P048_O004V', 'P048_O005V', 'P048_O006V', 'P049_O001V', 'P049_O002V', 'P049_O003V', 'P049_O004V', 'P049_O005V', 'P049_O006V', 'P049_O007V', 'P049_O008V', 'P049_O009V', 'P049_O010V', 'P049_O011V', 'P050_O002V', 'P050_O003V', 'P050_O004V', 'P050_O005V', 'P050_O006V', 'P050_O007V', 'P050_O008V', 'P050_O009V', 'P050_O010V', 'P050_O011V', 'P051_O001V', 'P051_O002V', 'P051_O003V', 'P051_O004V', 'P051_O005V', 'P052_O001V', 'P052_O002V', 'P052_O003V', 'P052_O004V', 'P052_O005V', 'P052_O006V', 'P052_O007V', 'P052_O008V', 'P052_O009V', 'P053_O001V', 'P054_O001V', 'P055_O001V', 'P055_O002V', 'P055_O003V', 'P055_O004V', 'P055_O005V', 'P056_O001V', 'P056_O002V', 'P056_O003V', 'P056_O004V', 'P056_O005V', 'P056_O006V', 'P056_O007V', 'P056_O008V', 'P056_O009V', 'P056_O010V', 'P057_O001V', 'P058_O001V', 'P059_O001V', 'P059_O002V', 'P060_O001V', 'DESERCION', 'SEMESTRE PLAN', 'SEMESTRE ABANDONO', 'ETNIA', 'ADEUDO', 'Latitud', 'Longitud', 'Distancia_Campus']\n\nColumnas int64 (9):\n['EDAD', 'PUNTAJE_ADM', 'P005_O001V', 'P012_O001V', 'P029_O001V', 'P033_O001V', 'P048_O001V', 'P050_O001V', 'CAMBIO_PROGRAMA']\n\nColumnas object (17):\n['EMPLID', 'GENERO', 'CARRERA', 'UNIDAD', 'P005_O002V', 'P006_O008V', 'P006_O009V', 'P008_O002V', 'P010_O002V', 'P018_O002V', 'P047_O008V', 'P052_O010V', 'P060_O002V', 'Año', 'ÚLTIMO PERIODO CURSADO', 'Ciudad', 'Estado']\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/4bf73adc-6431-45df-8209-5cdb833cd20e","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"62940ebf","execution_start":1752123162437,"execution_millis":84,"execution_context_id":"b4734e89-24fa-421e-a465-f05d90503f39","cell_id":"257b9fd42650419e93bd5e7979556221","deepnote_cell_type":"code"},"source":"# 1. Convertir una columna de tipo object a tipo string e imputar No aplica y Desconocido\n# Lista de columnas a convertir a tipo string\ncolumns_to_convert = [\n    'EMPLID', 'CARRERA', 'UNIDAD', 'GENERO', 'Ciudad',\n    'Estado', 'P052_O010V', 'P047_O008V', 'P006_O008V',\n    'P006_O009V', 'P010_O002V', 'P060_O002V', 'P005_O002V'\n]\n\n# Identificar valores perdidos en las columnas especificadas antes de la conversión\nmissing_values_before_conversion = df[columns_to_convert].isna().sum()\n\n# Mostrar valores perdidos antes de la conversión\nprint(\"\\nValores perdidos en las columnas especificadas antes de la conversión:\")\nprint(missing_values_before_conversion[missing_values_before_conversion > 0])\n\n# Imputación de valores para columnas específicas\n# Columnas a rellenar con \"No aplica\" por ser las opciones de \"Especifica\" (preguntas abiertas)\ncolumns_no_aplica = [\n    'P006_O008V', 'P006_O009V', 'P052_O010V', 'P047_O008V',\n    'P060_O002V', 'P010_O002V', 'P005_O002V'\n]\n\n# Columnas a rellenar con \"Desconocido\" \ncolumns_desconocido = [\n    'EMPLID', 'CARRERA', 'UNIDAD', 'GENERO', 'Ciudad', 'Estado'\n]\n\n# Rellenar valores nulos con \"No aplica\"\ndf[columns_no_aplica] = df[columns_no_aplica].fillna(\"No aplica\")\n\n# Rellenar valores nulos con \"Desconocido\"\ndf[columns_desconocido] = df[columns_desconocido].fillna(\"Desconocido\")\n\n# Convertir todas las columnas a tipo string después de la imputación\ndf[columns_to_convert] = df[columns_to_convert].astype('string')\n\n# Identificar valores imputados con \"No aplica\"\nconteo_no_aplica = (df[columns_no_aplica] == \"No aplica\").sum()\n\n# Identificar valores imputados con \"Desconocido\"\nconteo_desconocido = (df[columns_desconocido] == \"Desconocido\").sum()\n\n# Mostrar resultados\nprint(\"\\nImputación completada:\")\nprint(\"Total de valores imputados con 'No aplica' por columna (por ser las opciones de respueste en preguntas donde pide Especificar):\")\nprint(conteo_no_aplica[conteo_no_aplica > 0])\n\nprint(\"\\nTotal de valores imputados con 'Desconocido' por columna (valores nulos):\")\nprint(conteo_desconocido[conteo_desconocido > 0])\n\n# Verificar si todavía hay valores perdidos\nmissing_values_after_imputation = df[columns_to_convert].isna().sum()\nif missing_values_after_imputation.sum() > 0:\n    print(\"\\nAún hay valores perdidos en las columnas después de la imputación:\")\n    print(missing_values_after_imputation[missing_values_after_imputation > 0])\nelse:\n    print(\"\\nNo hay valores perdidos después de la imputación.\")","block_group":"c534b3dbb7f843b7adfde612be9d204b","execution_count":20,"outputs":[{"name":"stdout","text":"\nValores perdidos en las columnas especificadas antes de la conversión:\nCARRERA         35\nP052_O010V     150\nP047_O008V    3326\nP006_O008V      31\nP006_O009V    8092\nP010_O002V      71\nP060_O002V     236\nP005_O002V       7\ndtype: int64\n\nImputación completada:\nTotal de valores imputados con 'No aplica' por columna (por ser las opciones de respueste en preguntas donde pide Especificar):\nP006_O008V      31\nP006_O009V    8092\nP052_O010V     150\nP047_O008V    3326\nP060_O002V     236\nP010_O002V      71\nP005_O002V       7\ndtype: Int64\n\nTotal de valores imputados con 'Desconocido' por columna (valores nulos):\nCARRERA    35\ndtype: Int64\n\nNo hay valores perdidos después de la imputación.\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/99ad40bf-16c4-4ea8-9af6-503fb07c2fc8","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"2f76bece","execution_start":1752123162567,"execution_millis":1,"execution_context_id":"b4734e89-24fa-421e-a465-f05d90503f39","cell_id":"ddc41caca75348e9a4b0dace0b1407fa","deepnote_cell_type":"code"},"source":"# 2. Convertir la columna 'Año' a formato de fecha\n# Identificar y mostrar valores perdidos en la columna 'Año' antes de la conversión\nmissing_values_before = df['Año'].isna().sum()\nprint(f\"\\nValores perdidos en la columna 'Año' antes de la conversión: {missing_values_before}\")\n\n# Convertir la columna 'Año' a formato de fecha\ndf['Año'] = pd.to_datetime(df['Año'].astype(str) + '-01-01', format='%Y-%m-%d', errors='coerce').dt.year\n\n# Identificar y mostrar valores perdidos en la columna 'Año' después de la conversión\nmissing_values_after = df['Año'].isna().sum()\nprint(f\"\\nValores perdidos en la columna 'Año' después de la conversión: {missing_values_after}\")\n\n# Verificar el tipo de dato y las primeras filas\nprint(\"\\nTipo de dato de la columna 'Año':\", df['Año'].dtype)\nprint(\"\\nMuestra de las primeras filas de la columna 'Año' para confirmar:\")\nprint(df['Año'].head())","block_group":"8d9368ffc1a74ad0a4a802ef9abfb1aa","execution_count":21,"outputs":[{"name":"stdout","text":"\nValores perdidos en la columna 'Año' antes de la conversión: 0\n\nValores perdidos en la columna 'Año' después de la conversión: 0\n\nTipo de dato de la columna 'Año': int32\n\nMuestra de las primeras filas de la columna 'Año' para confirmar:\n0    2015\n1    2014\n2    2014\n3    2015\n4    2016\nName: Año, dtype: int32\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/12634dc8-e986-48f4-b84b-c003a2d301dd","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"bb7e3c01","execution_start":1752123162617,"execution_millis":3,"execution_context_id":"b4734e89-24fa-421e-a465-f05d90503f39","cell_id":"a59bbb783746448f81de05c63703b0db","deepnote_cell_type":"code"},"source":"import numpy as np\n\n# 3.Imputar con 0 las respuestas a las preguntas: P008_O002V (Cuantos cigarrillos fumas); P013_O001V (Cuantas horas trabajas); P018_O002V (Cuantos hijos tienes)\n\ncols_to_impute = ['P008_O002V', 'P013_O001V', 'P018_O002V']\nfor col in cols_to_impute:\n    # Verificar valores perdidos antes de la imputación\n    missing_before = df[col].isna().sum()\n    print(f\"\\n Columna '{col}':\")\n    print(f\"Valores perdidos antes de la imputación: {missing_before}\")\n    \n    # Mostrar valores únicos no nulos antes de imputar\n    print(f\"Valores únicos antes de imputar:\")\n    print(df[col].dropna().unique())\n\n    # Reemplazar valores no numéricos o vacíos por NaN\n    df[col] = df[col].replace(['---', 'N/A', 'n/a', 's/n', '', ' '], np.nan)\n\n    # Imputar valores NaN con 0\n    df[col] = df[col].fillna(0)\n\n    # Convertir a tipo numérico\n    df[col] = pd.to_numeric(df[col], errors='coerce')\n\n    # Verificar valores perdidos después de la imputación y conversión\n    missing_after = df[col].isna().sum()\n    print(f\"Valores perdidos después de la imputación: {missing_after}\")\n\n# Verificar los tipos de datos después de la conversión\nint64_columns = df.select_dtypes(include='int64').columns\nprint(f\"\\nColumnas int64 ({len(int64_columns)}): {int64_columns.tolist()}\")","block_group":"1a07c286b2824c51be82d61ee813b621","execution_count":22,"outputs":[{"name":"stdout","text":"\n Columna 'P008_O002V':\nValores perdidos antes de la imputación: 8107\nValores únicos antes de imputar:\n['---' '1' '2' '7' '64' '8' '4' '3' '03' '01' '0' '5' '10' '6' '02' '15'\n '9' '30' '20' '999' '40' '25' '56']\nValores perdidos después de la imputación: 0\n\n Columna 'P013_O001V':\nValores perdidos antes de la imputación: 180\nValores únicos antes de imputar:\n[ 16.    0.   24.   60.   40.   35.   20.    5.   42.   30.   50.   21.\n   4.   56.    6.   72.   12.   48.   28.   27.    8.   45.   18.   22.\n  32.   15.    9.   25.    2.    3.   36.   10.   68.   46.    7.   14.\n  58.   78.   23.   76.   54.   66.   85.   44.   49.   52.   55.   38.\n  84.   47.   63.   41.   33.   39.   26.   64.   70.   74.   99.   11.\n  98.   13.   17.   53.   80.   31.   51.   90.   75.   65.   29.   34.\n  43.    1.  800.  500.   67.   37.  200.   81.    9.6  73.   62.   57.\n 120.   19.   95.   77.   59.   71.   69.   88. ]\nValores perdidos después de la imputación: 0\n\n Columna 'P018_O002V':\nValores perdidos antes de la imputación: 1355\nValores únicos antes de imputar:\n['0' 1 0 '---' '1' '2' '3' '4' 2 '56' 3 4 '5' '28']\nValores perdidos después de la imputación: 0\n\nColumnas int64 (11): ['EDAD', 'PUNTAJE_ADM', 'P005_O001V', 'P008_O002V', 'P012_O001V', 'P018_O002V', 'P029_O001V', 'P033_O001V', 'P048_O001V', 'P050_O001V', 'CAMBIO_PROGRAMA']\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/c80de9c6-db38-46a6-b88f-a7fbd24b451a","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"956d0593","execution_start":1752123162667,"execution_millis":7,"execution_context_id":"b4734e89-24fa-421e-a465-f05d90503f39","cell_id":"d3e2f8e88ce943e2ab9429ab2392823b","deepnote_cell_type":"code"},"source":"# LIMPIEZA DE VALORES EXTREMOS (datos irreales o errores de captura en las tres columnas anteriores)\n\n# Función para limpiar valores extremos\ndef limpiar_valores_irreales(df):\n\n    # P008_O002V – cigarrillos por día\n    df['P008_O002V'] = df['P008_O002V'].astype(str).str.lstrip('0')  # eliminar ceros a la izquierda\n    df['P008_O002V'] = pd.to_numeric(df['P008_O002V'], errors='coerce')\n    df.loc[df['P008_O002V'] > 40, 'P008_O002V'] = np.nan  # umbral razonable\n    df['P008_O002V'] = df['P008_O002V'].fillna(0).astype(int)\n\n    # P013_O001V – horas trabajadas\n    df['P013_O001V'] = pd.to_numeric(df['P013_O001V'], errors='coerce')\n    df.loc[df['P013_O001V'] > 84, 'P013_O001V'] = np.nan  # máx 12 h/día x 7 días\n    df['P013_O001V'] = df['P013_O001V'].fillna(0).astype(int)\n\n    # P018_O002V – hijos\n    df['P018_O002V'] = df['P018_O002V'].astype(str).str.lstrip('0')\n    df['P018_O002V'] = pd.to_numeric(df['P018_O002V'], errors='coerce')\n    df.loc[df['P018_O002V'] > 10, 'P018_O002V'] = np.nan\n    df['P018_O002V'] = df['P018_O002V'].fillna(0).astype(int)\n\n    return df\n\n# Aplicar limpieza\ndf = limpiar_valores_irreales(df)\n\n# Verificar\nfor col in ['P008_O002V', 'P013_O001V', 'P018_O002V']:\n    print(f\"\\n{col} después de limpiar:\")\n    print(df[col].describe())\n","block_group":"d613ca258a13457e95b7e255e7fdd9d0","execution_count":23,"outputs":[{"name":"stdout","text":"\nP008_O002V después de limpiar:\ncount    38932.000000\nmean         0.062006\nstd          0.620316\nmin          0.000000\n25%          0.000000\n50%          0.000000\n75%          0.000000\nmax         40.000000\nName: P008_O002V, dtype: float64\n\nP013_O001V después de limpiar:\ncount    38932.000000\nmean         7.278717\nstd         15.262934\nmin          0.000000\n25%          0.000000\n50%          0.000000\n75%          6.000000\nmax         84.000000\nName: P013_O001V, dtype: float64\n\nP018_O002V después de limpiar:\ncount    38932.000000\nmean         0.033469\nstd          0.236711\nmin          0.000000\n25%          0.000000\n50%          0.000000\n75%          0.000000\nmax          5.000000\nName: P018_O002V, dtype: float64\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/82f6a146-57c8-471c-ab68-de4b0fa3fcc5","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"2297d3fe","execution_start":1752123162727,"execution_millis":388,"execution_context_id":"b4734e89-24fa-421e-a465-f05d90503f39","cell_id":"4ecaed3637d04ccea74cbc448b91ed4a","deepnote_cell_type":"code"},"source":"# 4. Rellenar Valores Nulos para el resto de las variables\n\n# Identificar columnas con valores nulos\ncolumnas_con_nulos = df.columns[df.isnull().any()]\n\n# Contadores por tipo de imputación\ncols_imputadas_cero = []\ncols_imputadas_NA = []\ncols_imputadas_desconocido = []\ncols_imputadas_moda = []\n\n# Guardar la columna ETNIA antes de la imputación\ndf_etnia = df[['ETNIA']].copy()\n\n# Rellenar valores nulos según grupo\nfor col in columnas_con_nulos:  \n    if col in ['P014_O001V', 'P015_O002V', 'P015_O001V', 'P015_O003V', 'P015_O004V',\n               'P015_O005V', 'P016_O001V', 'P043_O001V', 'P042_O001V', 'P027_O001V', 'P027_O002V',\n               'P027_O003V', 'P027_O004V', 'P027_O005V', 'P027_O006V', 'P027_O007V',\n               'P027_O008V', 'P027_O009V', 'P027_O010V', 'P027_O011V', 'P027_O012V',\n               'P027_O013V', 'P027_O014V', 'P026_O001V', 'P024_O001V', 'P024_O002V',\n               'P018_O001V', 'P044_O001V', 'P008_O001V', 'P009_O001V', 'P007_O001V',\n               'P010_O001V', 'P006_O001V', 'P006_O002V', 'P006_O003V', 'P006_O004V',\n               'P051_O001V', 'P051_O002V', 'P051_O003V', 'P051_O004V', 'P051_O005V',  \n               'P006_O005V', 'P006_O006V', 'P006_O007V', 'P019_O001V']:\n        df[col] = df[col].fillna(\"0\")\n        cols_imputadas_cero.append(col)\n\n    elif col in ['P011_O001V', 'P017_O001V', 'P039_O001V', 'P048_O001V', 'P048_O002V', \n                 'P048_O003V', 'P048_O004V', 'P048_O005V', 'P048_O006V']:\n        df[col] = df[col].fillna(\"NA\")\n        cols_imputadas_NA.append(col)\n\n    elif col in ['P057_O001V', 'P058_O001V', 'P059_O001V', 'P059_O002V', 'P060_O001V', \n                 'P020_O001V', 'P022_O001V']:\n        df[col] = df[col].fillna(\"Desconocido\")\n        cols_imputadas_desconocido.append(col)\n\n    else:\n        mode_value = df[col][df[col] != 0].mode(dropna=True).iloc[0] if not df[col][df[col] != 0].mode(dropna=True).empty else None\n        if mode_value is not None:\n            df[col] = df[col].fillna(mode_value)\n            cols_imputadas_moda.append(col)\n\n# Restaurar ETNIA\ndf['ETNIA'] = df_etnia['ETNIA']\n\n# Verificación\nprint(\"\\nTipos de imputación aplicados:\")\nprint(f\"Columnas imputadas con '0': {len(cols_imputadas_cero)} → {cols_imputadas_cero}\")\nprint(f\"Columnas imputadas con 'NA': {len(cols_imputadas_NA)} → {cols_imputadas_NA}\")\nprint(f\"Columnas imputadas con 'Desconocido': {len(cols_imputadas_desconocido)} → {cols_imputadas_desconocido}\")\nprint(f\"Columnas imputadas con moda (sin considerar 0): {len(cols_imputadas_moda)} → {cols_imputadas_moda}\")\n\n# Verificación de nulos\nprint(\"\\nValores nulos en 'ETNIA' después de la imputación (no debe cambiarse):\", df['ETNIA'].isna().sum())\n\nnulos_restantes = df.isna().sum()\nnulos_finales = nulos_restantes[nulos_restantes > 0]\n\nif nulos_finales.empty:\n    print(\"No hay columnas con valores nulos después de la imputación.\")\nelse:\n    print(\"Aún hay columnas con valores nulos:\")\n    print(nulos_finales)","block_group":"152c7f4e692c4f7e893e09ddd48d3a38","execution_count":24,"outputs":[{"name":"stdout","text":"\nTipos de imputación aplicados:\nColumnas imputadas con '0': 45 → ['P006_O001V', 'P006_O002V', 'P006_O003V', 'P006_O004V', 'P006_O005V', 'P006_O006V', 'P006_O007V', 'P007_O001V', 'P008_O001V', 'P009_O001V', 'P010_O001V', 'P014_O001V', 'P015_O001V', 'P015_O002V', 'P015_O003V', 'P015_O004V', 'P015_O005V', 'P016_O001V', 'P018_O001V', 'P019_O001V', 'P024_O001V', 'P024_O002V', 'P026_O001V', 'P027_O001V', 'P027_O002V', 'P027_O003V', 'P027_O004V', 'P027_O005V', 'P027_O006V', 'P027_O007V', 'P027_O008V', 'P027_O009V', 'P027_O010V', 'P027_O011V', 'P027_O012V', 'P027_O013V', 'P027_O014V', 'P042_O001V', 'P043_O001V', 'P044_O001V', 'P051_O001V', 'P051_O002V', 'P051_O003V', 'P051_O004V', 'P051_O005V']\nColumnas imputadas con 'NA': 8 → ['P011_O001V', 'P017_O001V', 'P039_O001V', 'P048_O002V', 'P048_O003V', 'P048_O004V', 'P048_O005V', 'P048_O006V']\nColumnas imputadas con 'Desconocido': 7 → ['P020_O001V', 'P022_O001V', 'P057_O001V', 'P058_O001V', 'P059_O001V', 'P059_O002V', 'P060_O001V']\nColumnas imputadas con moda (sin considerar 0): 128 → ['P021_O001V', 'P023_O001V', 'P025_O001V', 'P028_O001V', 'P030_O001V', 'P031_O001V', 'P031_O002V', 'P031_O003V', 'P032_O001V', 'P032_O002V', 'P032_O003V', 'P033_O002V', 'P033_O003V', 'P033_O004V', 'P033_O005V', 'P033_O006V', 'P033_O007V', 'P033_O008V', 'P033_O009V', 'P033_O010V', 'P034_O001V', 'P034_O002V', 'P034_O003V', 'P034_O004V', 'P034_O005V', 'P034_O006V', 'P034_O007V', 'P034_O008V', 'P034_O009V', 'P035_O001V', 'P035_O002V', 'P035_O003V', 'P035_O004V', 'P035_O005V', 'P036_O001V', 'P036_O002V', 'P036_O003V', 'P036_O004V', 'P036_O005V', 'P036_O006V', 'P036_O007V', 'P037_O001V', 'P037_O002V', 'P037_O003V', 'P037_O004V', 'P037_O005V', 'P037_O006V', 'P037_O007V', 'P037_O008V', 'P038_O001V', 'P040_O001V', 'P040_O002V', 'P041_O001V', 'P041_O002V', 'P041_O003V', 'P041_O004V', 'P041_O005V', 'P041_O006V', 'P045_O001V', 'P046_O001V', 'P046_O002V', 'P046_O003V', 'P046_O004V', 'P046_O005V', 'P046_O006V', 'P046_O007V', 'P046_O008V', 'P046_O009V', 'P046_O010V', 'P047_O001V', 'P047_O002V', 'P047_O003V', 'P047_O004V', 'P047_O005V', 'P047_O006V', 'P047_O007V', 'P049_O001V', 'P049_O002V', 'P049_O003V', 'P049_O004V', 'P049_O005V', 'P049_O006V', 'P049_O007V', 'P049_O008V', 'P049_O009V', 'P049_O010V', 'P049_O011V', 'P050_O002V', 'P050_O003V', 'P050_O004V', 'P050_O005V', 'P050_O006V', 'P050_O007V', 'P050_O008V', 'P050_O009V', 'P050_O010V', 'P050_O011V', 'P052_O001V', 'P052_O002V', 'P052_O003V', 'P052_O004V', 'P052_O005V', 'P052_O006V', 'P052_O007V', 'P052_O008V', 'P052_O009V', 'P053_O001V', 'P054_O001V', 'P055_O001V', 'P055_O002V', 'P055_O003V', 'P055_O004V', 'P055_O005V', 'P056_O001V', 'P056_O002V', 'P056_O003V', 'P056_O004V', 'P056_O005V', 'P056_O006V', 'P056_O007V', 'P056_O008V', 'P056_O009V', 'P056_O010V', 'SEMESTRE PLAN', 'ÚLTIMO PERIODO CURSADO', 'SEMESTRE ABANDONO', 'ETNIA', 'ADEUDO']\n\nValores nulos en 'ETNIA' después de la imputación (no debe cambiarse): 24835\nAún hay columnas con valores nulos:\nETNIA    24835\ndtype: int64\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/ec134026-1f38-4a36-acf2-4a895ef390b8","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"35a0b15c","execution_start":1752123163177,"execution_millis":0,"execution_context_id":"b4734e89-24fa-421e-a465-f05d90503f39","cell_id":"1246a33886dc4ccf89a4e0f3dbb937e3","deepnote_cell_type":"code"},"source":"# 4. Cambiar el dato numérico de las respuestas 'No' con valor de 2, a valor de 0 (columnas: 'P005_O001V', 'P007_O001V', 'P008_O001V', 'P012_O001V', 'P018_O001V', 'P026_O001V', 'P042_O001V', 'P043_O001V') para mantener consistencia\n\n# Lista de columnas que contienen las respuestas\ncolumnas_respuestas = ['P005_O001V', 'P007_O001V', 'P008_O001V', 'P012_O001V', \n                       'P018_O001V', 'P026_O001V', 'P042_O001V', 'P043_O001V',\n                       'P051_O001V', 'P051_O002V', 'P051_O003V', 'P051_O004V', 'P051_O005V']\n\n# Reemplazar los valores de 2 por 0 en las columnas especificadas\ndf[columnas_respuestas] = df[columnas_respuestas].replace(2, 0)\n\n# Verificar cuántas filas fueron afectadas por la actualización\nfilas_afectadas = df[columnas_respuestas].shape[0]\nprint(f\"Filas afectadas: {filas_afectadas}\")","block_group":"6bbab22ad29a47548f3d3fb49cb6002a","execution_count":25,"outputs":[{"name":"stdout","text":"Filas afectadas: 38932\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/f975daac-5ce3-4270-8b6f-a99064b4623c","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"f188f734","execution_start":1752123163227,"execution_millis":0,"execution_context_id":"b4734e89-24fa-421e-a465-f05d90503f39","cell_id":"aedf58a0fed24cd487841eb619abed3f","deepnote_cell_type":"code"},"source":"# 5. Cambiar el dato numerico de las respuestas 'No aplica' con valor de 5, a valor de 0 (columnas: 'P055_O001V', 'P055_O002V', P055_O003V', 'P055_O004V', 'P055_O005V') ##\n\n# Lista de columnas a modificar\ncolumnas = ['P055_O001V', 'P055_O002V', 'P055_O003V', 'P055_O004V', 'P055_O005V']\n\n# Cambiar el valor numérico 5 a 0 donde el valor es 'No aplica'\nfor columna in columnas:\n    df[columna] = df[columna].replace(5, 0)\n\n# Verificar cuántas filas fueron afectadas por la actualización\nfilas_afectadas = df[columnas].shape[0]\nprint(f\"Filas afectadas: {filas_afectadas}\")","block_group":"da18a3979f0747bcbbaebff3a35215c9","execution_count":26,"outputs":[{"name":"stdout","text":"Filas afectadas: 38932\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/5bd99614-195e-4761-9e87-d91e64d23f3c","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"27a5526f","execution_start":1752123163277,"execution_millis":0,"execution_context_id":"b4734e89-24fa-421e-a465-f05d90503f39","cell_id":"56d1aeef7b834cebb0caf84dd1eae8df","deepnote_cell_type":"code"},"source":"# 6. Cambiar el dato numérico de las respuestas 'No había' con valor de 5, a valor de 0 (preguntas: 'P048_O001V', 'P048_O002V', 'P048_O003V', 'P048_O004V', 'P048_O005V', 'P048_O006V') ##\n\n# Lista de columnas a actualizar\ncolumnas = ['P048_O001V', 'P048_O002V', 'P048_O003V', 'P048_O004V', 'P048_O005V', 'P048_O006V']\n\n# Cambiar valor 5 a 0 solo si el valor es 5 en las columnas especificadas\nfor columna in columnas:\n    df[columna] = df[columna].replace(5, 0)\n\n# Verificar cuántas filas fueron afectadas por la actualización\nfilas_afectadas = df[columnas].shape[0]\nprint(f\"Filas afectadas: {filas_afectadas}\")","block_group":"f706cb265267434cadbdf2ac57fa92fb","execution_count":27,"outputs":[{"name":"stdout","text":"Filas afectadas: 38932\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/a5ed9345-aeef-42b1-b199-842e4f803fbb","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"4fd617f3","execution_start":1752123163327,"execution_millis":0,"execution_context_id":"b4734e89-24fa-421e-a465-f05d90503f39","cell_id":"155db780605e4ff99710827544294529","deepnote_cell_type":"code"},"source":"# 7. Cambiar el dato numérico de las respuestas 'Lo ignoro' con valor de 13 a valor de 0\ncolumns_to_update = ['P024_O001V', 'P024_O002V']\n\nfor column in columns_to_update:\n    df[column] = df[column].replace(13, 0)\n\n# Verificar cuántas filas fueron afectadas por la actualización\nfilas_afectadas = df[columns_to_update].shape[0]\nprint(f\"Filas afectadas: {filas_afectadas}\")","block_group":"8f92f25a2d5e45b5b076371ea9f89890","execution_count":28,"outputs":[{"name":"stdout","text":"Filas afectadas: 38932\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/f7a53520-722a-43aa-be0a-a7ed55d94f33","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"ec61375a","execution_start":1752123163387,"execution_millis":0,"execution_context_id":"b4734e89-24fa-421e-a465-f05d90503f39","cell_id":"fa70a129f5db4d86b94136262964d9d6","deepnote_cell_type":"code"},"source":"# 8. Cambiar el dato numérico de las respuestas 'No aplica' con valor de -1, a valor de 0 (columnas: 'P014_O001V', 'P016_O001V')                                                                                                                                        # Lista de columnas a modificar\ncolumnas = ['P014_O001V', 'P016_O001V']\n\n# Cambiar el valor numérico 5 a 0 donde el valor es 'No aplica'\nfor columna in columnas:\n    df[columna] = df[columna].replace(-1, 0)\n\n# Verificar cuántas filas fueron afectadas por la actualización\nfilas_afectadas = df[columnas].shape[0]\nprint(f\"Filas afectadas: {filas_afectadas}\")","block_group":"6fec8545378f4b1589fb88cfd4ff4e51","execution_count":29,"outputs":[{"name":"stdout","text":"Filas afectadas: 38932\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/1cf1efbb-99f7-45e1-9df8-d5c490060e86","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"2f3f215","execution_start":1752123163447,"execution_millis":0,"execution_context_id":"b4734e89-24fa-421e-a465-f05d90503f39","cell_id":"84c0cdcdd1d0434d8daf0abc78224f56","deepnote_cell_type":"code"},"source":"# 9. AJUSTAR DATOS ABERRANTES EN EL CAMPO DE EDAD (EDAD > 80 o EDAD < 15)\n# 1) Calcular la moda de la columna EDAD \nmode_edad = df['EDAD'].mode(dropna=True)[0]  # Toma el primer valor si hay varias modas\n\n# 2) Contar cuántos outliers se van a imputar\noutliers_mask = (df['EDAD'] < 15) | (df['EDAD'] > 80)\nnum_outliers = df[outliers_mask].shape[0]\nprint(f\"Registros con valores de EDAD fuera de [15, 80]: {num_outliers}\")\n\n# 3) Reemplazar (imputar) la edad con la moda para esos registros\ndf.loc[outliers_mask, 'EDAD'] = mode_edad\n\n# 4) Verificar resultado\nprint(\"Número de registros después de imputar outliers:\", df.shape[0])\nprint(df['EDAD'].describe())","block_group":"3b2114fae80240fdae49944b3b4cd3d1","execution_count":30,"outputs":[{"name":"stdout","text":"Registros con valores de EDAD fuera de [15, 80]: 266\nNúmero de registros después de imputar outliers: 38932\ncount    38932.000000\nmean        18.940974\nstd          2.340537\nmin         15.000000\n25%         18.000000\n50%         18.000000\n75%         19.000000\nmax         71.000000\nName: EDAD, dtype: float64\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/d66d2f28-ddbf-462f-9c16-81d39b46abd4","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"b62d75c6","execution_start":1752123163507,"execution_millis":3003,"execution_context_id":"b4734e89-24fa-421e-a465-f05d90503f39","cell_id":"e7bae2d74d944c3f9e16cddb3e700e19","deepnote_cell_type":"code"},"source":"# CLASIFICACIÓN DE CARRERAS\n\n# Definir listas manuales\nprogramas_condicionados = [\"PLEIN\", \"PLPSIC\", \"PICIA\",\"PIIS \",\"PLA\",\"PLCP\",\"PINSO\",\"PIMEC\",\"PLEI\",\"PLAET\",\"PLCIE\",\"PLDG\",\"PLEF\",\"PIBS\",\"PIELM\",\"PLTAL\",\"PLARQ\",\"PIETR\",\"PIMAN\",\"PLGDA\",\"PPADI\",\"PMVZO\",\"PCIEF\",\"PLDCF\",\"PLED\"]  # Agregar más\n\n# Clasificación por condición (aceptado/condicionado) y nivel (lic, ing, etc.)\ndef clasificar_programa(codigo):\n    \"\"\"\n    Devuelve:\n    - CATEGORIA_CARRERA: Condicionado / Aceptado\n    - NIVEL_CARRERA: Licenciatura, Ingeniería, Profesional Asociado, Desconocido\n    - RISK_CARRERA: número arbitrario para representar mayor riesgo en condicionados\n    \"\"\"\n    codigo = str(codigo).strip().upper()\n\n    # Clasificación por condición\n    categoria = \"Condicionado\" if codigo in programas_condicionados else \"Aceptado\"\n    riesgo = 1 if categoria == \"Condicionado\" else 0\n\n    # Clasificación por nivel de carrera (puedes ajustar o expandir reglas)\n    if codigo.startswith(\"I\") or \"ING\" in codigo:\n        nivel = \"Engineering program\"\n    elif codigo.startswith(\"L\") or codigo.startswith(\"ARQ\") or codigo.startswith(\"MVZ\"):\n        nivel = \"Bachelor's degree\"\n    elif codigo.startswith(\"PA\"):\n        nivel = \"Associate degree\"\n    else:\n        nivel = \"Desconocido\"\n\n    return categoria, nivel, riesgo\n\n# Desfragmentar el DataFrame antes de agregar nuevas columnas\ndf = df.copy()\n\n# Aplicar clasificación a la columna CARRERA\ndf[['CATEGORIA_CARRERA', 'NIVEL_CARRERA', 'RISK_CARRERA']] = df['CARRERA'].astype(str).apply(\n    lambda x: pd.Series(clasificar_programa(x))\n)\n\n# Verificación resultado\nprint(f\"Columnas en el DataFrame: {df.shape[1]}\")\nprint(df[['CARRERA', 'CATEGORIA_CARRERA', 'NIVEL_CARRERA', 'RISK_CARRERA']].head())","block_group":"24670fe14e664918a23be5bc112edf79","execution_count":31,"outputs":[{"name":"stdout","text":"Columnas en el DataFrame: 222\n  CARRERA CATEGORIA_CARRERA        NIVEL_CARRERA  RISK_CARRERA\n0   LDCFD          Aceptado    Bachelor's degree             0\n1   LDCFD          Aceptado    Bachelor's degree             0\n2   MVZOO          Aceptado    Bachelor's degree             0\n3   LCOPU          Aceptado    Bachelor's degree             0\n4   IMECA          Aceptado  Engineering program             0\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/4e2ae1a5-43ce-464f-aa8e-82ff0e37293b","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"40a69070","execution_start":1752123166577,"execution_millis":351,"execution_context_id":"b4734e89-24fa-421e-a465-f05d90503f39","cell_id":"bb9a6dd976134a68b530af0d4d4ff8ac","deepnote_cell_type":"code"},"source":"# UNIFICACIÓN  DE CARRERAS\n\n# Diccionario de agrupación\nagrupaciones = {\n    \"LEI\": [\"LEIN\", \"LEI\", \"PLEIN\", \"PLEI\"],\n    \"IIS\": [\"PIIS\", \"IISIS\"],\n    \"LA\": [\"PLA\", \"LADMI\"],\n    \"LPS\": [\"LPSIC\", \"LPSC\", \"PLPSI\"],\n    \"LCOPU\": [\"LCP\", \"PLCP\"],\n    \"ISW\": [\"INSOF\", \"PINSO\"],\n    \"IC\": [\"ICIVI\", \"PICIA\", \"PIC\"],\n    \"IEM\": [\"PIMEC\", \"IMECA\"], \n    \"IELME\": [\"PIELM\"],      \n    \"LCE\": [\"LCIED\", \"PLED\"], \n    \"MVZ\": [\"MVZOO\", \"PMVZO\"], \n    \"LAET\": [\"LADTU\", \"PLAET\"], \n    \"LCEF\": [\"LCIEF\", \"PLCIE\", \"PCIEF\"], \n    \"LDG\": [\"LDIGR\", \"PLDG\"], \n    \"LEF\": [\"LECFI\", \"PLEF\"], \n    \"IB\": [\"IBIOT\"], \n    \"IQ\": [\"IQUIM\", \"PIQ\"], \n    \"IBS\": [\"IBIOS\", \"PIBS\"], \n    \"ICA\": [\"ICIAM\"], \n    \"LTA\": [\"LTALI\", \"PLTAL\"], \n    \"ARQ\": [\"LARQ\", \"PLARQ\"], \n    \"IE\": [\"IETRO\", \"PIETR\"], \n    \"LDCFD\": [\"PLDCF\"], \n    \"IMAN\": [\"PIMAN\"], \n    \"LGDA\": [\"LGDAR\", \"PLGDA\"], \n    \"PADIN\": [\"PPADI\"],\n    # Programas sin grupo\n    \"LENF\": [\"LENF\"], \"LG\": [\"LG\"], \"LEM\": [\"LEM\"], \"IL\": [\"IL\"], \"LEAGC\": [\"LEAGC\"], \"LEIGI\": [\"LEIGI\"], \n    \"PAAI\": [\"PAAI\"], \"LDER\": [\"LDER\"], \"LAES\": [\"LAES\"]\n}\n\n# Función para unificar nombres de carrera\ndef unificar_nombre(carrera):\n    carrera = str(carrera).strip().upper()\n    for nombre_unificado, variantes in agrupaciones.items():\n        if carrera in [v.strip().upper() for v in variantes] or carrera == nombre_unificado:\n            return nombre_unificado\n    return carrera\n\n# Función para clasificar área disciplinar\ndef clasificar_area(carrera):\n    carrera = str(carrera).strip().upper()\n    ingenierias = ['LDG','ISW','IE','IIS','IC','ARQ','IEM','IL','PIMAN','INSOF','PLDG','PLARQ','PINSO','PIMEC','PMAN','PIIS','PIETR','PIELM','PICIA','PIC','PAAI','LDIGR','LARQ','IMAN','IISIS','IETRO','IELME']\n    sociales_humanidades = ['LPS','LCE','LCEF','PLGDA','LGDA','PLED','LPSC','LEIN','LEIGI','LEAGC','LDER','PPADI','PLPSI','PLEIN','PLEI','PLDCF','PLCIE','PCIEF','PADIN','LEI','LDCFD','LCIED','LPSIC','LADMI']\n    naturales = ['LTA','IQ','IBS','ICA''IB','LENF','PMVZO','PLTAL','PIQ','PIBS','MVZ','LTALI','IQUIM','ICIAM','IBIOT','IBIOS']\n    economico_admin = ['LEF','LA','LAET','LEM','LCP','LAES','PLEF','PLCP','PLAET','PLA','LG','LECFI','LCOPU','LCIEF','LADTU']\n\n    if carrera in ingenierias:\n        return 'Engineering and Technology'\n    elif carrera in sociales_humanidades:\n        return 'Social Sciences and Humanities'\n    elif carrera in naturales:\n        return 'Natural Resources'\n    elif carrera in economico_admin:\n        return 'Business & Economics'\n    else:\n        return 'Desconocido'\n\n# Aplicar funciones\ncarrera_unificada = df[\"CARRERA\"].astype(str).apply(unificar_nombre)\narea_carrera = carrera_unificada.apply(clasificar_area)\n\n# Reemplazar columna y evitar duplicados\ncols_to_drop = [c for c in ['CARRERA', 'AREA_CARRERA'] if c in df.columns]\ndf = pd.concat([\n    df.drop(columns=cols_to_drop),\n    carrera_unificada.rename(\"CARRERA\"),\n    area_carrera.rename(\"AREA_CARRERA\")\n], axis=1)\n\n# Desfragmentar el DataFrame para evitar warnings de rendimiento\ndf = df.copy()\n\n# Verificación\nprint(\"\\nVerificación de unificación de carrera y clasificación disciplinar:\")\nprint(df[['CARRERA', 'AREA_CARRERA']].head())","block_group":"129a11dd74ce48f7ac810b8115d6accb","execution_count":32,"outputs":[{"name":"stdout","text":"\nVerificación de unificación de carrera y clasificación disciplinar:\n  CARRERA                    AREA_CARRERA\n0   LDCFD  Social Sciences and Humanities\n1   LDCFD  Social Sciences and Humanities\n2     MVZ               Natural Resources\n3   LCOPU            Business & Economics\n4     IEM      Engineering and Technology\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/8e2f341b-603b-4606-82b1-b1e7a2a70333","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"60828ace","execution_start":1752123166977,"execution_millis":0,"execution_context_id":"b4734e89-24fa-421e-a465-f05d90503f39","cell_id":"b6f5e9ba52d249788c275f31d0845918","deepnote_cell_type":"code"},"source":"# MUESTRA TOTAL DE REGISTROS EN EL DATAFRAME ##\n\n# Contar registros por año\nconteo_por_anio = df['Año'].value_counts().sort_index()\n\n# Mostrar resultados\nprint(\"Conteo de registros por año en el dataframe:\")\nprint(conteo_por_anio)","block_group":"f34da23a1f344537aa9bd279bf0d419d","execution_count":33,"outputs":[{"name":"stdout","text":"Conteo de registros por año en el dataframe:\nAño\n2014    3726\n2015    4335\n2016    4004\n2017    3949\n2018    4069\n2019    4983\n2020    4433\n2021     787\n2022    3964\n2023    3938\n2024     744\nName: count, dtype: int64\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/bb8e8b99-e62c-45cc-8aed-63dcd9694f40","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"a6f87415","execution_start":1752123167037,"execution_millis":3921,"execution_context_id":"b4734e89-24fa-421e-a465-f05d90503f39","cell_id":"9766b21bcdf84db9b1fee361fed9f4c5","deepnote_cell_type":"code"},"source":"# ACTUALIZAR DF CON CAMBIOS (RECODIFICACION Y TRATAMIENTO DE VALORES NULOS) ##\n# Guardar el dataframe combinado en un archivo nuevo\ndf.to_csv(os.path.join(folder_path, 'combined.csv'), index=False)\n\nprint(f\"Columnas en el DataFrame: {df.shape[1]}\")","block_group":"644784f3a2a648dbaa1021a4c7f624cb","execution_count":34,"outputs":[{"name":"stdout","text":"Columnas en el DataFrame: 223\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/68896948-f156-4c58-8559-aee225558ec4","content_dependencies":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=24fa97d8-6b94-4f62-a2dc-97416a953ae1' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_notebook_id":"fd20fb1211c649d288f487a12d640656"}}