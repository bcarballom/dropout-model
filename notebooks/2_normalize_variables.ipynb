{"cells":[{"cell_type":"code","metadata":{"source_hash":"8a83ec31","execution_start":1752123333807,"execution_millis":1752,"execution_context_id":"f18a2060-bf09-4db4-ac54-4bc10b704bec","cell_id":"a18098fe4c5645eea520e9394dac9a76","deepnote_cell_type":"code"},"source":"# 1) CARGAR EL DATAFRAME COMBINADO Y DETECTAR COLUMNAS CON MEZCLA DE TIPOS\n\nimport pandas as pd\n\n# Cargar con low_memory=False para evitar advertencias durante la inferencia de tipos\ndf = pd.read_csv('data/combined.csv', low_memory=False)\n\n# Verificar el dataframe cargado:\nfilas_cargadas = df.shape[0]  # obtener el número de filas cargadas\nprint(f\"DataFrame cargado: {filas_cargadas} filas.\")\nprint(f\"Columnas en el DataFrame: {df.shape[1]}\")\n\n# Detectar columnas con mezcla de tipos usando 'object' que deberían ser numéricas\nmixed_type_cols = []\n\nfor col in df.columns:\n    # Si tiene más de un tipo de dato (ej. int y str)\n    tipos = df[col].map(type).value_counts()\n    if len(tipos) > 1:\n        mixed_type_cols.append((col, tipos))\n\n# Mostrar un resumen\nif mixed_type_cols:\n    print(\"Columnas con tipos de datos mezclados detectadas:\")\n    for col, tipos in mixed_type_cols:\n        print(f\"Columna: '{col}' - Tipos detectados: {tipos.to_dict()}\")\nelse:\n    print(\"No se detectaron columnas con tipos de datos mezclados.\")","block_group":"804d20ba27624c58894982c9da0bbc9a","execution_count":1,"outputs":[{"name":"stdout","text":"DataFrame cargado: 38932 filas.\nColumnas en el DataFrame: 223\nNo se detectaron columnas con tipos de datos mezclados.\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/71e8c90d-13a0-4f2b-bcf4-450f14a703d1","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"621b0572","execution_start":1752123335617,"execution_millis":1,"execution_context_id":"f18a2060-bf09-4db4-ac54-4bc10b704bec","cell_id":"4921e7352b40484da10f3bd42ad6f5d1","deepnote_cell_type":"code"},"source":"# FASE 3. NORMALIZACION O ESTANDARIZACION (PREPARACION PARA EL MACHINE LEARNING)\n\n#  0) VARIABLES CATEGORICAS (CADENAS DE TEXTO)\n\n# GENERO: En la realidad de ITSON las mujeres (F) desertan menos que los hombres (M), por lo que se esperaría que tienen menor riesgo de deserción.\ndf['GENERO'] = df['GENERO'].astype(str).str.upper().map({'F': 0, 'M': 1})\n\n# CARRERA: Se cree que un alumno condicionado tiene mayor riesgo de deserción.\n\n# Verificación resultado\nprint(f\"Columnas en el DataFrame: {df.shape[1]}\")\nprint(df[['GENERO', 'CARRERA', 'RISK_CARRERA']].head())","block_group":"3a2692f48eee47ff9b99a0726b669c2e","execution_count":2,"outputs":[{"name":"stdout","text":"Columnas en el DataFrame: 223\n   GENERO CARRERA  RISK_CARRERA\n0     1.0   LDCFD             0\n1     0.0   LDCFD             0\n2     0.0     MVZ             0\n3     0.0   LCOPU             0\n4     1.0     IEM             0\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/495b36b3-41cd-42c5-b8cf-2c299ca4bfea","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"957d3295","execution_start":1752123335677,"execution_millis":107,"execution_context_id":"f18a2060-bf09-4db4-ac54-4bc10b704bec","cell_id":"90082e3d4c3f4fd9b1ae86771564a177","deepnote_cell_type":"code"},"source":"import numpy as np\n\n# 1. Convertir una columna de tipo object a tipo string e imputar No aplica y Desconocido\n# Lista de columnas a convertir a tipo string\ncolumns_to_convert = [\n    'EMPLID', 'CARRERA', 'UNIDAD', 'Ciudad', \n    'Estado', 'P052_O010V', 'P047_O008V', 'P006_O008V',\n    'P006_O009V', 'P010_O002V', 'P060_O002V', 'P005_O002V'\n]\n\n# Imputación de valores para columnas específicas (ya imputadas en el notebook anterior)\n# Columnas a rellenar con \"No aplica\" por ser las opciones de \"Especifica\" (preguntas abiertas)\ncolumns_no_aplica = [\n    'P006_O008V', 'P006_O009V', 'P052_O010V', 'P047_O008V',\n    'P060_O002V', 'P010_O002V', 'P005_O002V'\n]\n\n# Columnas a rellenar con \"Desconocido\" \ncolumns_desconocido = [\n    'EMPLID', 'CARRERA', 'UNIDAD', 'GENERO', 'Ciudad', 'Estado'\n]\n\n# Rellenar valores nulos con \"No aplica\"\ndf[columns_no_aplica] = df[columns_no_aplica].fillna(\"No aplica\")\n\n# Rellenar valores nulos con \"Desconocido\"\ndf[columns_desconocido] = df[columns_desconocido].fillna(\"Desconocido\")\n\n# Convertir todas las columnas a tipo string después de la imputación\ndf[columns_to_convert] = df[columns_to_convert].astype('string')\n\n\n# 2.Imputar con 0 las respuestas a las preguntas: P008_O002V (Cuantos cigarrillos fumas); P013_O001V (Cuantas horas trabajas); P018_O002V (Cuantos hijos tienes)\n\ncols_to_impute = ['P008_O002V', 'P013_O001V', 'P018_O002V']\nfor col in cols_to_impute:\n    # Verificar valores perdidos antes de la imputación\n    missing_before = df[col].isna().sum()\n    \n    # Mostrar valores únicos no nulos antes de imputar\n\n    # Reemplazar valores no numéricos o vacíos por NaN\n    df[col] = df[col].replace(['---', 'N/A', 'n/a', 's/n', '', ' '], np.nan)\n\n    # Imputar valores NaN con 0\n    df[col] = df[col].fillna(0)\n\n    # Convertir a tipo numérico\n    df[col] = pd.to_numeric(df[col], errors='coerce')\n\n# 3. Rellenar Valores Nulos para el resto de las variables\n\n# Identificar columnas con valores nulos\ncolumnas_con_nulos = df.columns[df.isnull().any()]\n\n# Contadores por tipo de imputación\ncols_imputadas_cero = []\ncols_imputadas_NA = []\ncols_imputadas_desconocido = []\ncols_imputadas_moda = []\n\n# Guardar la columna ETNIA antes de la imputación\ndf_etnia = df[['ETNIA']].copy()\n\n# Rellenar valores nulos según grupo\nfor col in columnas_con_nulos:  \n    if col in ['P014_O001V', 'P015_O002V', 'P015_O001V', 'P015_O003V', 'P015_O004V',\n               'P015_O005V', 'P016_O001V', 'P043_O001V', 'P042_O001V', 'P027_O001V', 'P027_O002V',\n               'P027_O003V', 'P027_O004V', 'P027_O005V', 'P027_O006V', 'P027_O007V',\n               'P027_O008V', 'P027_O009V', 'P027_O010V', 'P027_O011V', 'P027_O012V',\n               'P027_O013V', 'P027_O014V', 'P026_O001V', 'P024_O001V', 'P024_O002V',\n               'P018_O001V', 'P044_O001V', 'P008_O001V', 'P009_O001V', 'P007_O001V',\n               'P010_O001V', 'P006_O001V', 'P006_O002V', 'P006_O003V', 'P006_O004V', \n               'P006_O005V', 'P006_O006V', 'P006_O007V', 'P019_O001V']:\n        df[col] = df[col].fillna(\"0\")\n        cols_imputadas_cero.append(col)\n\n    elif col in ['P051_O001V', 'P051_O002V', 'P051_O003V', 'P051_O004V', 'P051_O005V', \n                 'P011_O001V', 'P017_O001V', 'P039_O001V', 'P048_O001V', 'P048_O002V', \n                 'P048_O003V', 'P048_O004V', 'P048_O005V', 'P048_O006V']:\n        df[col] = df[col].fillna(\"NA\")\n        cols_imputadas_NA.append(col)\n\n    elif col in ['P057_O001V', 'P058_O001V', 'P059_O001V', 'P059_O002V', 'P060_O001V', \n                 'P020_O001V', 'P022_O001V']:\n        df[col] = df[col].fillna(\"Desconocido\")\n        cols_imputadas_desconocido.append(col)\n\n    else:\n        mode_value = df[col][df[col] != 0].mode(dropna=True).iloc[0] if not df[col][df[col] != 0].mode(dropna=True).empty else None\n        if mode_value is not None:\n            df[col] = df[col].fillna(mode_value)\n            cols_imputadas_moda.append(col)\n\n# Restaurar ETNIA\ndf['ETNIA'] = df_etnia['ETNIA']\n\n# Verificación\nnulos_restantes = df.isna().sum()\nnulos_finales = nulos_restantes[nulos_restantes > 0]\n\nif nulos_finales.empty:\n    print(\"No hay columnas con valores nulos después de la imputación.\")\nelse:\n    print(\"Aún hay columnas con valores nulos:\")\n    print(nulos_finales)","block_group":"ab62af2a5f214dedac106f284aa984dc","execution_count":3,"outputs":[{"name":"stdout","text":"Aún hay columnas con valores nulos:\nETNIA    24835\ndtype: int64\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/88c074bc-e3bc-401f-bcbf-e3e5e805990a","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"c27b9e56","execution_start":1752123335827,"execution_millis":11,"execution_context_id":"f18a2060-bf09-4db4-ac54-4bc10b704bec","cell_id":"3b92889ff5244f419048b0644f4ff5fd","deepnote_cell_type":"code"},"source":"# 1) VARIABLES CUALITATIVAS NOMINALES A CODIFICAR CON ONE HOT ENCODING\n\n# Lista  de variables categóricas nominales (con varias respuestas)\nvariables_categoricas = list(set([\n    'P009_O001V', 'P010_O001V', 'P014_O001V', 'P017_O001V', 'P020_O001V', 'P022_O001V', 'P025_O001V', 'P031_O001V', 'P031_O002V', 'P031_O003V', \n    'P032_O001V', 'P032_O002V', 'P032_O003V', 'P053_O001V', \n    'P054_O001V', 'P058_O001V', 'P060_O001V', \n]))\n\n# Verificar valores nulos solo en las variables categóricas seleccionadas\nvalores_nulos_categoricas = df[variables_categoricas].isnull().sum()\ncolumnas_con_nulos_categoricas = valores_nulos_categoricas[valores_nulos_categoricas > 0]\n\n# Mensaje claro según el resultado\nif columnas_con_nulos_categoricas.empty:\n    print(\"Las variables categóricas seleccionadas para One Hot Encoding no tienen valores nulos.\")\nelse:\n    print(\"Las siguientes variables categóricas tienen valores nulos y podrían causar errores en One Hot Encoding:\")\n    print(columnas_con_nulos_categoricas)\n\n# Conteo de otros valores nulos en el dataframe\nvalores_nulos = df.isnull().sum()\ncolumnas_con_nulos = valores_nulos[valores_nulos > 0]\nif columnas_con_nulos.empty:\n    print(\"No hay columnas con valores nulos en otras columnas del DataFrame\")\nelse:\n    print(\"Número de valores nulos en otras columnas del DataFrame:\")\n    print(columnas_con_nulos)","block_group":"19f1cf479e2b46b3ab32bf47ee645729","execution_count":4,"outputs":[{"name":"stdout","text":"Las variables categóricas seleccionadas para One Hot Encoding no tienen valores nulos.\nNúmero de valores nulos en otras columnas del DataFrame:\nETNIA    24835\ndtype: int64\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/fb4b3a13-9eff-439b-a3d2-2f7f562127dc","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"71bcbd94","execution_start":1752123335897,"execution_millis":1026,"execution_context_id":"f18a2060-bf09-4db4-ac54-4bc10b704bec","cell_id":"210a2323b369442aa7e60a7085c7e8f9","deepnote_cell_type":"code"},"source":"# Contar número de columnas antes del One-Hot Encoding\nnum_columnas_antes = df.shape[1]\nprint(f\"Número de columnas antes del One-Hot Encoding: {num_columnas_antes}\")\n\n# Guardar nombres de columnas antes del One-Hot Encoding\ncolumnas_antes = set(df.columns)\n\n# Codificar las variables categóricas (One-Hot Encoding)\nvariables_categoricas_presentes = [col for col in variables_categoricas if col in df.columns]\n\nfor columna in variables_categoricas_presentes:\n    dummies = pd.get_dummies(df[columna], prefix=columna, drop_first=False)\n    df = pd.concat([df, dummies], axis=1)\n    df.drop(columna, axis=1, inplace=True)\n\n# Guardar nombres de columnas después del One-Hot Encoding\ncolumnas_despues = set(df.columns)\n\n# Determinar columnas nuevas creadas por el One-Hot Encoding\ncolumnas_creadas = columnas_despues - columnas_antes\n\nprint(f\"Columnas nuevas creadas ({len(columnas_creadas)}):\")\nprint(sorted(columnas_creadas))\n\n# Contar número de columnas después del One-Hot Encoding\nnum_columnas_despues = df.shape[1]\nnuevas_columnas = num_columnas_despues - num_columnas_antes\nprint(f\"Número de columnas nuevas agregadas (no repetidas): {nuevas_columnas}\")\nprint(f\"Número total de columnas después del One-Hot Encoding: {num_columnas_despues}\")\n\n# Verificar valores nulos después de la codificación\nvalores_nulos_actualizados = df.isnull().sum()\nvalores_nulos_presentes = valores_nulos_actualizados[valores_nulos_actualizados > 0]\n\nprint(\"\\nNúmero de valores nulos en el DataFrame después de la codificación:\")\nprint(valores_nulos_presentes if not valores_nulos_presentes.empty else \"No hay valores nulos.\")","block_group":"d4de319e622549bc8fd39ab8e1f08b71","execution_count":5,"outputs":[{"name":"stdout","text":"Número de columnas antes del One-Hot Encoding: 223\nColumnas nuevas creadas (80):\n['P009_O001V_0.0', 'P009_O001V_1.0', 'P009_O001V_2.0', 'P009_O001V_3.0', 'P009_O001V_4.0', 'P009_O001V_5.0', 'P009_O001V_6.0', 'P010_O001V_0.0', 'P010_O001V_1.0', 'P010_O001V_2.0', 'P010_O001V_3.0', 'P010_O001V_4.0', 'P010_O001V_5.0', 'P014_O001V_0.0', 'P014_O001V_1.0', 'P014_O001V_2.0', 'P014_O001V_3.0', 'P014_O001V_4.0', 'P014_O001V_5.0', 'P014_O001V_6.0', 'P017_O001V_1.0', 'P017_O001V_2.0', 'P017_O001V_3.0', 'P017_O001V_4.0', 'P017_O001V_5.0', 'P017_O001V_NA', 'P020_O001V_1.0', 'P020_O001V_2.0', 'P020_O001V_3.0', 'P020_O001V_4.0', 'P020_O001V_5.0', 'P020_O001V_6.0', 'P020_O001V_Desconocido', 'P022_O001V_1.0', 'P022_O001V_2.0', 'P022_O001V_3.0', 'P022_O001V_4.0', 'P022_O001V_5.0', 'P022_O001V_6.0', 'P022_O001V_7.0', 'P022_O001V_8.0', 'P022_O001V_Desconocido', 'P025_O001V_0.0', 'P025_O001V_1.0', 'P025_O001V_2.0', 'P025_O001V_3.0', 'P031_O001V_1.0', 'P031_O001V_2.0', 'P031_O001V_3.0', 'P031_O002V_1.0', 'P031_O002V_2.0', 'P031_O002V_3.0', 'P031_O003V_1.0', 'P031_O003V_2.0', 'P031_O003V_3.0', 'P032_O001V_1.0', 'P032_O001V_2.0', 'P032_O002V_1.0', 'P032_O002V_2.0', 'P032_O003V_1.0', 'P032_O003V_2.0', 'P053_O001V_1.0', 'P053_O001V_2.0', 'P053_O001V_3.0', 'P054_O001V_1.0', 'P054_O001V_2.0', 'P054_O001V_3.0', 'P058_O001V_1.0', 'P058_O001V_2.0', 'P058_O001V_3.0', 'P058_O001V_4.0', 'P058_O001V_5.0', 'P058_O001V_Desconocido', 'P060_O001V_1.0', 'P060_O001V_2.0', 'P060_O001V_3.0', 'P060_O001V_4.0', 'P060_O001V_5.0', 'P060_O001V_6.0', 'P060_O001V_Desconocido']\nNúmero de columnas nuevas agregadas (no repetidas): 63\nNúmero total de columnas después del One-Hot Encoding: 286\n\nNúmero de valores nulos en el DataFrame después de la codificación:\nETNIA    24835\ndtype: int64\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/4d9aef4d-2261-4053-8842-30bcf69a6f2e","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"641fa1a3","execution_start":1752123336967,"execution_millis":264,"execution_context_id":"f18a2060-bf09-4db4-ac54-4bc10b704bec","cell_id":"9922bbad20bf434b9c4f96e7f73e76f7","deepnote_cell_type":"code"},"source":"# Buscar columnas candidatas para conversión a entero\ncolumnas_a_convertir = []\n\nfor col in df.columns:\n    valores_unicos = df[col].dropna().unique()\n    \n    # Si todos los valores son booleanos, 0, 1 o '0', '1', se considera dummy\n    if set(valores_unicos).issubset({0, 1, True, False, '0', '1'}):\n        columnas_a_convertir.append(col)\n\n# Convertir a entero\nfor col in columnas_a_convertir:\n    # Reemplazar valores nulos con un valor predeterminado antes de la conversión\n    df[col] = df[col].fillna(0).astype(int)\n\n# Verificación\nprint(f\"Se convirtieron {len(columnas_a_convertir)} columnas a enteros.\")\nprint(df[columnas_a_convertir].dtypes)","block_group":"25fdd2df622347f3b577a739edf7e694","execution_count":6,"outputs":[{"name":"stdout","text":"Se convirtieron 183 columnas a enteros.\nP005_O001V                int64\nP006_O001V                int64\nP006_O002V                int64\nP006_O003V                int64\nP006_O004V                int64\n                          ...  \nP020_O001V_3.0            int64\nP020_O001V_4.0            int64\nP020_O001V_5.0            int64\nP020_O001V_6.0            int64\nP020_O001V_Desconocido    int64\nLength: 183, dtype: object\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/3a82b3c3-b483-42dd-92bf-55264ec74631","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"d6c0ac30","execution_start":1752123337277,"execution_millis":71,"execution_context_id":"f18a2060-bf09-4db4-ac54-4bc10b704bec","cell_id":"779134615a5348fc8ca1198cc0f023f3","deepnote_cell_type":"code"},"source":"# 2) VARIABLES CUALITATIVAS ORDINALES\n\n# Definir las columnas con las variables ordinales \ncolumns_to_check = list(set([\n    'P011_O001V', 'P016_O001V', 'P019_O001V', 'P021_O001V', 'P023_O001V', 'P024_O001V', \n    'P024_O002V', 'P028_O001V', 'P029_O001V', 'P030_O001V', 'P033_O001V', \n    'P033_O002V', 'P033_O003V', 'P033_O004V', 'P033_O005V', 'P033_O006V', 'P033_O007V',\n    'P033_O008V', 'P033_O009V', 'P033_O010V',\n    'P034_O001V', 'P034_O002V', 'P034_O003V', 'P034_O004V', 'P034_O005V', 'P034_O006V', \n    'P034_O007V', 'P034_O008V', 'P034_O009V', 'P035_O001V', 'P035_O002V', 'P035_O003V', \n    'P035_O004V', 'P035_O005V', 'P036_O001V', 'P036_O002V', 'P036_O003V', 'P036_O004V', \n    'P036_O005V', 'P036_O006V', 'P036_O007V', 'P038_O001V', 'P040_O001V', 'P040_O002V', \n    'P045_O001V', 'P048_O001V', 'P048_O002V', 'P048_O003V', 'P048_O004V',\n    'P048_O005V', 'P048_O006V', 'P049_O001V', 'P049_O002V', 'P049_O003V', 'P049_O004V', \n    'P049_O005V', 'P049_O006V', 'P049_O007V', 'P049_O008V', 'P049_O009V', 'P049_O010V', \n    'P049_O011V', 'P055_O001V', 'P055_O002V', 'P055_O003V', 'P055_O004V', 'P055_O005V',\n    'P057_O001V', 'P059_O001V', 'P059_O002V', 'P039_O001V'\n]))\n\n# 1. Verificar valores nulos exclusivamente en esas columnas\nnulos_ordinales = df[columns_to_check].isnull().sum()\ncolumnas_con_nulos_ordinales = nulos_ordinales[nulos_ordinales > 0]\n\n# Mensaje  según el resultado\nif columnas_con_nulos_ordinales.empty:\n    print(\"Las variables ordinales no tienen valores nulos.\")\nelse:\n    print(\"Las siguientes variables ordinales tienen valores nulos y podrían requerir imputación:\")\n    print(columnas_con_nulos_ordinales)\n\n# Conteo de otros valores nulos en el dataframe\nvalores_nulos = df.isnull().sum()\ncolumnas_con_nulos = valores_nulos[valores_nulos > 0]\nif columnas_con_nulos.empty:\n    print(\"No hay columnas con valores nulos en otras columnas del DataFrame\")\nelse:\n    print(\"Número de valores nulos en otras columnas del DataFrame:\")\n    print(columnas_con_nulos)","block_group":"9c92604d0e704a8098e9e45fe29a85e3","execution_count":7,"outputs":[{"name":"stdout","text":"Las variables ordinales no tienen valores nulos.\nNo hay columnas con valores nulos en otras columnas del DataFrame\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/7d1adc53-ba22-461a-a26a-3f81b279af8a","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"5b2e3592","execution_start":1752123337397,"execution_millis":57,"execution_context_id":"f18a2060-bf09-4db4-ac54-4bc10b704bec","cell_id":"15f6efba29464dd18be95a99eec2f4ee","deepnote_cell_type":"code"},"source":"# 2. Reemplazar 'NA', 'Desconocido', o cualquier valor no válido por un valor válido (moda)\nvariables_sin_mapeo = list(set([\n    'P011_O001V', # ¿Con qué frecuencia has perdido la noción del tiempo y después no recuerdas nada?\n    'P039_O001V', # ¿Cómo valoras el desempeño de tus profesores del bachillerato?\n    'P048_O002V', 'P048_O003V', 'P048_O004V', 'P048_O005V', 'P048_O006V', # ¿Con qué frecuencia utilizabas en el bachillerato los siguientes servicios?\n    'P057_O001V', # ¿Cómo consideras tus posibilidades de encontrar trabajo relacionado con tu profesión?\n    'P059_O001V', 'P059_O002V', # ¿Cómo percibes el desarrollo de tu vida profesional, una vez que concluyas tus estudios de licenciatura?\n]))\n\nfor col in variables_sin_mapeo:\n    if col in df.columns:\n        if df[col].dtype == 'object':\n            # Reemplaza 'NA' y 'Desconocido' con pd.NA\n            df[col] = df[col].replace(['NA', 'Desconocido'], pd.NA)\n            moda = df[col].mode(dropna=True)\n            if not moda.empty:\n                df[col] = df[col].fillna(moda[0])\n            else:\n                df[col] = df[col].fillna(\"Otro\")\n        else:\n            # Asegura tipo numérico y reemplaza NaN con la moda\n            df[col] = pd.to_numeric(df[col], errors='coerce')\n            moda = df[col].mode(dropna=True)\n            if not moda.empty:\n                df[col] = df[col].fillna(moda[0])\n            else:\n                df[col] = df[col].fillna(0)\n\n# Verificar los valores únicos después del reemplazo\nprint(df[variables_sin_mapeo].isnull().sum())\nprint(\"\\nValores únicos en las columnas después de reemplazo:\")\nfor column in variables_sin_mapeo:\n    print(f\"{column}: {df[column].unique()}\")","block_group":"72feaf63f25b44888b9edec6d306122f","execution_count":8,"outputs":[{"name":"stdout","text":"P048_O004V    0\nP057_O001V    0\nP048_O003V    0\nP048_O006V    0\nP011_O001V    0\nP039_O001V    0\nP059_O002V    0\nP059_O001V    0\nP048_O002V    0\nP048_O005V    0\ndtype: int64\n\nValores únicos en las columnas después de reemplazo:\nP048_O004V: [1. 2. 3. 0. 4.]\nP057_O001V: ['1.0' '2.0' '3.0' '4.0']\nP048_O003V: [1. 2. 3. 4. 0.]\nP048_O006V: [1. 0. 3. 2. 4.]\nP011_O001V: [1. 4. 2. 6. 3. 5.]\nP039_O001V: [1. 2. 3. 4.]\nP059_O002V: ['3.0' '2.0' '1.0' '4.0' '5.0']\nP059_O001V: ['3.0' '2.0' '1.0' '4.0' '5.0']\nP048_O002V: [1. 3. 4. 0. 2.]\nP048_O005V: [1. 2. 3. 4. 0.]\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/feb9a48d-db67-40b1-9c94-00cdc25fcdea","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"d012877f","execution_start":1752123337507,"execution_millis":0,"execution_context_id":"f18a2060-bf09-4db4-ac54-4bc10b704bec","cell_id":"e8261e603e3046e395ff667a92c3d9fa","deepnote_cell_type":"code"},"source":"# 3. Verificar el rango de los valores en las variables ordinales\nfor column in columns_to_check:\n    df[column] = pd.to_numeric(df[column], errors='coerce')  # Convierte a numérico, y pone NaN en valores no convertibles\nprint(\"\\nVerificación del rango de valores en las columnas ordinales después de la conversión:\")\nfor column in columns_to_check:\n    min_value = df[column].min()\n    max_value = df[column].max()\n    print(f\"{column}: Min = {min_value}, Max = {max_value}\")","block_group":"c66ea909b39446489d59b308393af892","execution_count":9,"outputs":[{"name":"stdout","text":"\nVerificación del rango de valores en las columnas ordinales después de la conversión:\nP049_O007V: Min = 1.0, Max = 4.0\nP048_O004V: Min = 0.0, Max = 4.0\nP034_O004V: Min = 1.0, Max = 4.0\nP057_O001V: Min = 1.0, Max = 4.0\nP059_O001V: Min = 1.0, Max = 5.0\nP033_O001V: Min = 1, Max = 4\nP048_O001V: Min = 0, Max = 4\nP033_O003V: Min = 1.0, Max = 4.0\nP049_O008V: Min = 1.0, Max = 4.0\nP023_O001V: Min = 1.0, Max = 5.0\nP024_O002V: Min = 0.0, Max = 12.0\nP036_O001V: Min = 1.0, Max = 4.0\nP036_O006V: Min = 1.0, Max = 4.0\nP049_O002V: Min = 1.0, Max = 4.0\nP055_O004V: Min = 0.0, Max = 4.0\nP033_O006V: Min = 1.0, Max = 4.0\nP040_O001V: Min = 1.0, Max = 4.0\nP016_O001V: Min = 0.0, Max = 4.0\nP049_O009V: Min = 1.0, Max = 4.0\nP034_O006V: Min = 1.0, Max = 4.0\nP028_O001V: Min = 1.0, Max = 3.0\nP035_O004V: Min = 1.0, Max = 4.0\nP049_O005V: Min = 1.0, Max = 4.0\nP034_O003V: Min = 1.0, Max = 4.0\nP034_O001V: Min = 1.0, Max = 4.0\nP055_O005V: Min = 0.0, Max = 4.0\nP030_O001V: Min = 1.0, Max = 3.0\nP035_O005V: Min = 1.0, Max = 4.0\nP036_O004V: Min = 1.0, Max = 4.0\nP024_O001V: Min = 0.0, Max = 12.0\nP029_O001V: Min = 1, Max = 4\nP033_O008V: Min = 1.0, Max = 4.0\nP035_O003V: Min = 1.0, Max = 4.0\nP040_O002V: Min = 1.0, Max = 4.0\nP049_O011V: Min = 1.0, Max = 4.0\nP059_O002V: Min = 1.0, Max = 5.0\nP036_O002V: Min = 1.0, Max = 4.0\nP036_O005V: Min = 1.0, Max = 4.0\nP049_O004V: Min = 1.0, Max = 4.0\nP055_O002V: Min = 0.0, Max = 4.0\nP036_O007V: Min = 1.0, Max = 4.0\nP049_O010V: Min = 1.0, Max = 4.0\nP033_O009V: Min = 1.0, Max = 4.0\nP033_O010V: Min = 1.0, Max = 4.0\nP048_O003V: Min = 0.0, Max = 4.0\nP049_O003V: Min = 1.0, Max = 4.0\nP011_O001V: Min = 1.0, Max = 6.0\nP039_O001V: Min = 1.0, Max = 4.0\nP036_O003V: Min = 1.0, Max = 4.0\nP033_O002V: Min = 1.0, Max = 4.0\nP049_O006V: Min = 1.0, Max = 4.0\nP038_O001V: Min = 1.0, Max = 4.0\nP035_O002V: Min = 1.0, Max = 4.0\nP055_O001V: Min = 0.0, Max = 4.0\nP048_O005V: Min = 0.0, Max = 4.0\nP034_O009V: Min = 1.0, Max = 4.0\nP021_O001V: Min = 1.0, Max = 3.0\nP034_O002V: Min = 1.0, Max = 4.0\nP035_O001V: Min = 1.0, Max = 4.0\nP034_O008V: Min = 1.0, Max = 4.0\nP019_O001V: Min = 0.0, Max = 4.0\nP034_O005V: Min = 1.0, Max = 4.0\nP048_O006V: Min = 0.0, Max = 4.0\nP049_O001V: Min = 1.0, Max = 4.0\nP055_O003V: Min = 0.0, Max = 4.0\nP045_O001V: Min = 1.0, Max = 4.0\nP033_O005V: Min = 1.0, Max = 4.0\nP033_O007V: Min = 1.0, Max = 4.0\nP048_O002V: Min = 0.0, Max = 4.0\nP033_O004V: Min = 1.0, Max = 4.0\nP034_O007V: Min = 1.0, Max = 4.0\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/6a36a5b8-a0d1-4fef-b268-1aa6103a951e","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"d7dcf38c","execution_start":1752123337567,"execution_millis":500,"execution_context_id":"f18a2060-bf09-4db4-ac54-4bc10b704bec","cell_id":"73ac64459b5246c1bbbbde49986bbf02","deepnote_cell_type":"code"},"source":"# 4. Importar y crear un objeto MinMaxScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Crear un objeto MinMaxScaler\nscaler = MinMaxScaler()\n\n# Aplicar el MinMaxScaler solo a las columnas ordinales\ndf[columns_to_check] = scaler.fit_transform(df[columns_to_check])\n\n# Verificar los primeros registros para confirmar la normalización\nprint(df[columns_to_check].head())\n\n# Verificar los resultados después de la codificación ordinal\nprint(\"\\nVerificación de los valores después de Ordinal Encoding:\")\nfor column in columns_to_check:\n    print(f\"{column}: {df[column].unique()}\")","block_group":"4c47a5cb74d44b0ea1fa98482e2d9a12","execution_count":10,"outputs":[{"name":"stdout","text":"   P049_O007V  P048_O004V  P034_O004V  P057_O001V  P059_O001V  P033_O001V  \\\n0    1.000000        0.25    1.000000    0.000000        0.50         0.0   \n1    0.666667        0.25    1.000000    0.000000        0.25         0.0   \n2    0.000000        0.25    0.333333    0.333333        0.50         0.0   \n3    1.000000        0.25    0.666667    0.333333        0.25         0.0   \n4    0.666667        0.50    0.666667    0.000000        0.00         0.0   \n\n   P048_O001V  P033_O003V  P049_O008V  P023_O001V  ...  P034_O005V  \\\n0        0.25    0.333333    0.666667        1.00  ...    0.333333   \n1        0.25    0.000000    0.333333        0.25  ...    0.666667   \n2        0.25    0.000000    0.000000        0.00  ...    0.666667   \n3        0.25    0.333333    1.000000        0.50  ...    0.666667   \n4        0.75    0.333333    1.000000        0.25  ...    0.666667   \n\n   P048_O006V  P049_O001V  P055_O003V  P045_O001V  P033_O005V  P033_O007V  \\\n0        0.25    1.000000        0.25    0.000000    0.666667    0.000000   \n1        0.25    0.666667        0.25    0.000000    0.000000    0.666667   \n2        0.25    0.333333        0.25    1.000000    1.000000    0.000000   \n3        0.25    1.000000        0.25    0.000000    0.333333    0.666667   \n4        0.00    1.000000        0.25    0.333333    0.000000    0.000000   \n\n   P048_O002V  P033_O004V  P034_O007V  \n0        0.25    0.666667    0.000000  \n1        0.25    0.000000    0.666667  \n2        0.25    0.333333    0.000000  \n3        0.25    0.000000    0.666667  \n4        0.75    0.000000    0.666667  \n\n[5 rows x 71 columns]\n\nVerificación de los valores después de Ordinal Encoding:\nP049_O007V: [1.         0.66666667 0.         0.33333333]\nP048_O004V: [0.25 0.5  0.75 0.   1.  ]\nP034_O004V: [1.         0.33333333 0.66666667 0.        ]\nP057_O001V: [0.         0.33333333 0.66666667 1.        ]\nP059_O001V: [0.5  0.25 0.   0.75 1.  ]\nP033_O001V: [0.         0.33333333 1.         0.66666667]\nP048_O001V: [0.25 0.75 0.   0.5  1.  ]\nP033_O003V: [0.33333333 0.         0.66666667 1.        ]\nP049_O008V: [0.66666667 0.33333333 0.         1.        ]\nP023_O001V: [1.   0.25 0.   0.5  0.75]\nP024_O002V: [0.5        0.25       0.91666667 0.58333333 0.41666667 0.33333333\n 0.75       0.66666667 0.         0.83333333 1.         0.08333333\n 0.16666667]\nP036_O001V: [0.         0.33333333 1.         0.66666667]\nP036_O006V: [0.         0.66666667 0.33333333 1.        ]\nP049_O002V: [0.66666667 0.         1.         0.33333333]\nP055_O004V: [0.25 0.75 0.5  1.   0.  ]\nP033_O006V: [0.         0.66666667 0.33333333 1.        ]\nP040_O001V: [0.         0.66666667 0.33333333 1.        ]\nP016_O001V: [0.25 0.   0.75 1.   0.5 ]\nP049_O009V: [0.66666667 0.33333333 0.         1.        ]\nP034_O006V: [0.         0.33333333 0.66666667 1.        ]\nP028_O001V: [0.5 0.  1. ]\nP035_O004V: [0.66666667 0.33333333 0.         1.        ]\nP049_O005V: [1.         0.66666667 0.         0.33333333]\nP034_O003V: [0.         0.33333333 0.66666667 1.        ]\nP034_O001V: [1.         0.66666667 0.         0.33333333]\nP055_O005V: [0.25 0.5  1.   0.75 0.  ]\nP030_O001V: [0.  0.5 1. ]\nP035_O005V: [0.33333333 0.         0.66666667 1.        ]\nP036_O004V: [0.33333333 1.         0.66666667 0.        ]\nP024_O001V: [0.5        0.         0.91666667 0.33333333 0.41666667 0.58333333\n 0.66666667 0.25       1.         0.75       0.83333333 0.16666667\n 0.08333333]\nP029_O001V: [0.33333333 1.         0.         0.66666667]\nP033_O008V: [0.66666667 0.         0.33333333 1.        ]\nP035_O003V: [0.33333333 0.         0.66666667 1.        ]\nP040_O002V: [0.         0.33333333 0.66666667 1.        ]\nP049_O011V: [0.         1.         0.33333333 0.66666667]\nP059_O002V: [0.5  0.25 0.   0.75 1.  ]\nP036_O002V: [0.         0.66666667 1.         0.33333333]\nP036_O005V: [0.         0.66666667 0.33333333 1.        ]\nP049_O004V: [0.33333333 0.66666667 1.         0.        ]\nP055_O002V: [0.25 0.5  0.75 0.   1.  ]\nP036_O007V: [0.         0.33333333 0.66666667 1.        ]\nP049_O010V: [1.         0.33333333 0.66666667 0.        ]\nP033_O009V: [1.         0.33333333 0.66666667 0.        ]\nP033_O010V: [1.         0.         0.66666667 0.33333333]\nP048_O003V: [0.25 0.5  0.75 1.   0.  ]\nP049_O003V: [1.         0.         0.66666667 0.33333333]\nP011_O001V: [0.  0.6 0.2 1.  0.4 0.8]\nP039_O001V: [0.         0.33333333 0.66666667 1.        ]\nP036_O003V: [0.         0.66666667 0.33333333 1.        ]\nP033_O002V: [0.33333333 0.         0.66666667 1.        ]\nP049_O006V: [1.         0.66666667 0.33333333 0.        ]\nP038_O001V: [1.         0.66666667 0.33333333 0.        ]\nP035_O002V: [0.66666667 0.33333333 0.         1.        ]\nP055_O001V: [0.25 0.5  1.   0.75 0.  ]\nP048_O005V: [0.25 0.5  0.75 1.   0.  ]\nP034_O009V: [0.         0.33333333 0.66666667 1.        ]\nP021_O001V: [0.5 0.  1. ]\nP034_O002V: [0.33333333 0.         0.66666667 1.        ]\nP035_O001V: [0.         0.33333333 0.66666667 1.        ]\nP034_O008V: [0.         0.66666667 0.33333333 1.        ]\nP019_O001V: [0.25 0.5  1.   0.75 0.  ]\nP034_O005V: [0.33333333 0.66666667 1.         0.        ]\nP048_O006V: [0.25 0.   0.75 0.5  1.  ]\nP049_O001V: [1.         0.66666667 0.33333333 0.        ]\nP055_O003V: [0.25 0.5  1.   0.75 0.  ]\nP045_O001V: [0.         1.         0.33333333 0.66666667]\nP033_O005V: [0.66666667 0.         1.         0.33333333]\nP033_O007V: [0.         0.66666667 0.33333333 1.        ]\nP048_O002V: [0.25 0.75 1.   0.   0.5 ]\nP033_O004V: [0.66666667 0.         0.33333333 1.        ]\nP034_O007V: [0.         0.66666667 0.33333333 1.        ]\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/9d2e6688-cced-4dec-9068-61f3616acd03","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"960741a3","execution_start":1752123338117,"execution_millis":84,"execution_context_id":"f18a2060-bf09-4db4-ac54-4bc10b704bec","cell_id":"d0cae024412e4bb4bd44cf6e235170c3","deepnote_cell_type":"code"},"source":"#  3) VARIABLES CUANTITATIVAS\n### Normalización (rango 0 a 1) de variables cuantitativas\n\n# Asegurar consistencia en P013_O001V: si NO trabaja, entonces horas trabajadas = 0\ndf.loc[df['P012_O001V'] == '0', 'P013_O001V'] = 0\ndf['P013_O001V'] = pd.to_numeric(df['P013_O001V'], errors='coerce').fillna(0)\ndf.drop(columns=['P012_O001V'], inplace=True)\n\n# Desfragmentar el DataFrame antes de agregar nuevas columnas\ndf = df.copy()\n\n# Copiar columnas originales a preservar\ndf['EDAD_ESCALADA'] = df['EDAD']\ndf['BACH_PROMEDIO_ESCALADO'] = df['BACH_PROMEDIO']\n\n# Lista de variables cuantitativas a normalizar\nvariables_cuantitativas = ['P013_O001V', 'P044_O001V', 'P018_O002V', 'P008_O002V',\n                           'Distancia_Campus', 'BACH_PROMEDIO_ESCALADO', 'PUNTAJE_ADM', 'EDAD_ESCALADA']\n\n# Crear el objeto MinMaxScaler\nscaler = MinMaxScaler()\n\n# Aplicar la normalización\ndf[variables_cuantitativas] = scaler.fit_transform(df[variables_cuantitativas])\n\n# Mostrar verificación\nprint(\"\\nPrimeras filas con columnas normalizadas:\")\nprint(df[variables_cuantitativas].head())","block_group":"70d3c193c0e54119b953e8949ca8b2c2","execution_count":11,"outputs":[{"name":"stdout","text":"\nPrimeras filas con columnas normalizadas:\n   P013_O001V  P044_O001V  P018_O002V  P008_O002V  Distancia_Campus  \\\n0    0.190476    0.005722         0.0         0.0          0.010148   \n1    0.000000    0.001431         0.2         0.0          0.010148   \n2    0.000000    0.014306         0.0         0.0          0.704880   \n3    0.285714    0.007153         0.0         0.0          0.010148   \n4    0.714286    0.002861         0.0         0.0          0.010148   \n\n   BACH_PROMEDIO_ESCALADO  PUNTAJE_ADM  EDAD_ESCALADA  \n0                   0.819     0.397619       0.214286  \n1                   0.000     0.471429       0.285714  \n2                   0.740     0.504464       0.071429  \n3                   0.859     0.580952       0.053571  \n4                   0.809     0.466369       0.053571  \n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/a49cdb64-87c1-4000-b6b4-4a46124c02be","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"224348f3","execution_start":1752123338247,"execution_millis":0,"execution_context_id":"f18a2060-bf09-4db4-ac54-4bc10b704bec","cell_id":"9c4816b648fc4a9f97f1ad13fa12b05c","deepnote_cell_type":"code"},"source":"# Asegurar que no haya valores nulos en las variables \nvalores_nulos = df.isnull().sum()\ncolumnas_con_nulos = valores_nulos[valores_nulos > 0]\nif columnas_con_nulos.empty:\n    print(\"No hay columnas con valores nulos en el DataFrame\")\nelse:\n    print(\"Número de valores nulos en cada columna con valores nulos:\")\n    print(columnas_con_nulos)","block_group":"4fcbbba678aa4a7ba03412b311df6c38","execution_count":12,"outputs":[{"name":"stdout","text":"No hay columnas con valores nulos en el DataFrame\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/4238a32d-9e4a-4d38-836d-ef61ba08be74","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"1ddc8f9e","execution_start":1752123338297,"execution_millis":0,"execution_context_id":"f18a2060-bf09-4db4-ac54-4bc10b704bec","cell_id":"26b3339504144e6d8f118008ef4364c1","deepnote_cell_type":"code"},"source":"# Primero convertir las columnas numéricas a tipo float\nnumeric_columns = ['P051_O001V', 'P051_O002V', 'P051_O003V', 'P051_O004V', 'P051_O005V']\nfor col in numeric_columns:\n    df[col] = pd.to_numeric(df[col], errors='coerce')\n\n# Reemplazar valores nulos con la moda de cada columna\nfor col in numeric_columns:\n    df[col].fillna(df[col].mode(), inplace=True)\n\n# Reemplazar valores nulos con la moda de cada columna\ndf['GENERO'] = df['GENERO'].replace('nan', df['GENERO'].mode()[0])\n\n# Verificar nuevamente los valores nulos para confirmar el reemplazo\nvalores_nulos = df.isnull().sum()\ncolumnas_con_nulos = valores_nulos[valores_nulos > 0]\n\nif columnas_con_nulos.empty:\n    print(\"No hay columnas con valores nulos en el DataFrame\")\nelse:\n    print(\"Número de valores nulos en cada columna con valores nulos:\")\n    print(columnas_con_nulos)","block_group":"c47155f47e024d4ba19f9e7be0624454","execution_count":13,"outputs":[{"name":"stdout","text":"No hay columnas con valores nulos en el DataFrame\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/e66fe76c-1f0b-4826-9dcf-869a187423f1","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"8014cd1e","execution_start":1752123338347,"execution_millis":0,"execution_context_id":"f18a2060-bf09-4db4-ac54-4bc10b704bec","cell_id":"784d4afa479e4b20b309eac225f10699","deepnote_cell_type":"code"},"source":"# BORRAR COLUMNAS QUE NO SE NORMALIZARON\n\n# Contar columnas antes de la eliminación\ncolumnas_antes = df.shape[1]\n\n# Lista de columnas a eliminar \ncolumnas_a_eliminar = [\n    \"P005_O002V\", \"P006_O008V\", \"P006_O009V\", \"P010_O002V\", # Preguntas abiertas\n    \"P047_O008V\", \"P052_O010V\", \"P060_O002V\",  # Preguntas abiertas\n    \"PUNTAJE_ADM\",  # Porque cambió la escala cuando ya no se aplica examen de admisión\n    \"ADEUDO\"        # Porque solo se tiene para los estudiantes que desertaron\n]\n\n# Eliminar las columnas del DataFrame\ndf.drop(columns=columnas_a_eliminar, inplace=True, errors='ignore')\n\n# Contar columnas después de la eliminación\ncolumnas_despues = df.shape[1]\n\n# Imprimir resultado\nprint(f\"Se eliminaron {columnas_antes - columnas_despues} columnas.\")","block_group":"3b2a28595b1a4fc1b6e8ecff72b8b007","execution_count":14,"outputs":[{"name":"stdout","text":"Se eliminaron 9 columnas.\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/5a9d519a-dfd5-45c5-81b5-fa21e6d11151","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"e84fdf64","execution_start":1752123338407,"execution_millis":3879,"execution_context_id":"f18a2060-bf09-4db4-ac54-4bc10b704bec","cell_id":"8e003523820844b7a5070b11c5cde2dd","deepnote_cell_type":"code"},"source":"import os\n\n# Guardar el dataframe combinado en un archivo nuevo\nfolder_path = './data'  \ndf.to_csv(os.path.join(folder_path, 'encoded.csv'), index=False)\n\n# Verificar\nfilas_cargadas = df.shape[0] \nprint(f\"DataFrame guardado: {filas_cargadas} filas.\")\nprint(f\"Columnas en el DataFrame: {df.shape[1]}\")","block_group":"c7e0d7bc59c24d68acd8e4120fd7a5d4","execution_count":15,"outputs":[{"name":"stdout","text":"DataFrame guardado: 38932 filas.\nColumnas en el DataFrame: 278\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/75e11a77-e2a5-48fb-b039-7e0251388a9e","content_dependencies":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=24fa97d8-6b94-4f62-a2dc-97416a953ae1' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_persisted_session":{"createdAt":"2025-07-10T05:56:58.443Z"},"deepnote_notebook_id":"ef98426c2a1644a8a8060ccfc2cb22d7"}}